{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Sentiment Analysis Web App\n",
    "## Using PyTorch and SageMaker\n",
    "\n",
    "_Deep Learning Nanodegree Program | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have a basic understanding of how SageMaker works we will try to use it to construct a complete project from end to end. Our goal will be to have a simple web page which a user can use to enter a movie review. The web page will then send the review off to our deployed model which will predict the sentiment of the entered review.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this notebook. You will not need to modify the included code beyond what is requested. Sections that begin with '**TODO**' in the header indicate that you need to complete or implement some portion within them. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `# TODO: ...` comment. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions for you to answer which relate to the task and your implementation. Each section where you will answer a question is preceded by a '**Question:**' header. Carefully read each question and provide your answer below the '**Answer:**' header by editing the Markdown cell.\n",
    "\n",
    "> **Note**: Code and Markdown cells can be executed using the **Shift+Enter** keyboard shortcut. In addition, a cell can be edited by typically clicking it (double-click for Markdown cells) or by pressing **Enter** while it is highlighted.\n",
    "\n",
    "## General Outline\n",
    "\n",
    "Recall the general outline for SageMaker projects using a notebook instance.\n",
    "\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n",
    "\n",
    "For this project, you will be following the steps in the general outline with some modifications. \n",
    "\n",
    "First, you will not be testing the model in its own step. You will still be testing the model, however, you will do it by deploying your model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that you can make sure that your deployed model is working correctly before moving forward.\n",
    "\n",
    "In addition, you will deploy and use your trained model a second time. In the second iteration you will customize the way that your trained model is deployed by including some of your own code. In addition, your newly deployed model will be used in the sentiment analysis web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the data\n",
    "\n",
    "As in the XGBoost in SageMaker notebook, we will be using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2019-11-30 00:19:29--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  14.0MB/s    in 7.4s    \n",
      "\n",
      "2019-11-30 00:19:37 (10.9 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing and Processing the data\n",
    "\n",
    "Also, as in the XGBoost notebook, we will be doing some initial data processing. The first few steps are the same as in the XGBoost example. To begin with, we will read in each of the reviews and combine them into a single input structure. Then, we will split the dataset into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've read the raw training and testing data from the downloaded dataset, we will combine the positive and negative reviews and shuffle the resulting records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training and testing sets unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well I just gave away 95 minutes and 47 seconds that I'll never get back on this piece of trash. I heard someone online describe this movie's villains as \"subhuman cannibals\", and I thought it was promising because I thought it would be like the Descent. WRONG! The Descent was a psychological thriller with dynamic characters and strong storyline. These villains are totally unrealistic and no part of their performance is enjoyable to watch. This movie isn't so controversial, I've seen this level of gore in many films. This movie plain sucks. SYNOPSIS: A blonde who thinks she's real hot (but she isn't), her admirer, and her admirer's friend (no, I don't remember their names) go into the woods. Their car breaks down. They are warned to leave by a man named Mark. The blonde gets unreasonably hysterical and the next morning they can't find the admirer's friend. Admirer impales his foot (whoops!). Don't worry, he is much more upset when his car won't start than when he gets impaled by nails. After a nanosecond of coaxing, the blond leaves to find help. Events ensue that I cannot remember. During this and throughout the movie, we are shown grotesque torture scenes with no substance including one that made me gag. Blonde goes to save admirer from house of cannibals (even though all they are seen eating is intestines, which would logically be the last choice for real cannibals to eat since they contain actual food). Blonde finds admirer hurt and works very hard (unsuccessfully) to work up tears. Then you get a good laugh when the blonde is in the house and announces she can \"out think them\". Mark (the man who warned them to leave) has a remarkable change of character when he reveals the cannibals are his family. Then there is some shooting, they leave the house, the shooting continues, then a random guy shows up and says he's been watching them. Before he is shot, we are shown an acid-trip inspired scene of more killing. The blonde or her admirer shoots him because he did not help them. There's more killing, the admirer professes his love for the blonde. Then a mysterious hand covers the camera. What does that imply? I don't know, hopefully not a sequel.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in processing the reviews is to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as *entertained* and *entertaining* are considered the same with regard to sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `review_to_words` method defined above uses `BeautifulSoup` to remove any html tags that appear and uses the `nltk` package to tokenize the reviews. As a check to ensure we know how everything is working, try applying `review_to_words` to one of the reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'gave', 'away', '95', 'minut', '47', 'second', 'never', 'get', 'back', 'piec', 'trash', 'heard', 'someon', 'onlin', 'describ', 'movi', 'villain', 'subhuman', 'cannib', 'thought', 'promis', 'thought', 'would', 'like', 'descent', 'wrong', 'descent', 'psycholog', 'thriller', 'dynam', 'charact', 'strong', 'storylin', 'villain', 'total', 'unrealist', 'part', 'perform', 'enjoy', 'watch', 'movi', 'controversi', 'seen', 'level', 'gore', 'mani', 'film', 'movi', 'plain', 'suck', 'synopsi', 'blond', 'think', 'real', 'hot', 'admir', 'admir', 'friend', 'rememb', 'name', 'go', 'wood', 'car', 'break', 'warn', 'leav', 'man', 'name', 'mark', 'blond', 'get', 'unreason', 'hyster', 'next', 'morn', 'find', 'admir', 'friend', 'admir', 'impal', 'foot', 'whoop', 'worri', 'much', 'upset', 'car', 'start', 'get', 'impal', 'nail', 'nanosecond', 'coax', 'blond', 'leav', 'find', 'help', 'event', 'ensu', 'cannot', 'rememb', 'throughout', 'movi', 'shown', 'grotesqu', 'tortur', 'scene', 'substanc', 'includ', 'one', 'made', 'gag', 'blond', 'goe', 'save', 'admir', 'hous', 'cannib', 'even', 'though', 'seen', 'eat', 'intestin', 'would', 'logic', 'last', 'choic', 'real', 'cannib', 'eat', 'sinc', 'contain', 'actual', 'food', 'blond', 'find', 'admir', 'hurt', 'work', 'hard', 'unsuccess', 'work', 'tear', 'get', 'good', 'laugh', 'blond', 'hous', 'announc', 'think', 'mark', 'man', 'warn', 'leav', 'remark', 'chang', 'charact', 'reveal', 'cannib', 'famili', 'shoot', 'leav', 'hous', 'shoot', 'continu', 'random', 'guy', 'show', 'say', 'watch', 'shot', 'shown', 'acid', 'trip', 'inspir', 'scene', 'kill', 'blond', 'admir', 'shoot', 'help', 'kill', 'admir', 'profess', 'love', 'blond', 'mysteri', 'hand', 'cover', 'camera', 'impli', 'know', 'hope', 'sequel']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Apply review_to_words to a review (train_X[100] or any other review)\n",
    "print(review_to_words(train_X[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Above we mentioned that `review_to_words` method removes html formatting and allows us to tokenize the words found in a review, for example, converting *entertained* and *entertaining* into *entertain* so that they are treated as though they are the same word. What else, if anything, does this method do to the input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  It turns each token word into a string and puts them into a list.  Tokenization involves removing punctuation, verb tenses, capitalization, spaces, and html tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below applies the `review_to_words` method to each of the reviews in the training and testing datasets. In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data\n",
    "\n",
    "In the XGBoost notebook we transformed the data from its word representation to a bag-of-words feature representation. For the model we are going to construct in this notebook we will construct a feature representation which is very similar. To start, we will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely don't contain much information for the purposes of sentiment analysis. The way we will deal with this problem is that we will fix the size of our working vocabulary and we will only include the words that appear most frequently. We will then combine all of the infrequent words into a single category and, in our case, we will label it as `1`.\n",
    "\n",
    "Since we will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, we will fix a size for our reviews and then pad short reviews with the category 'no word' (which we will label `0`) and truncate long reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Create a word dictionary\n",
    "\n",
    "To begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the 'no word' and 'infrequent' categories) to be `5000` but you may wish to change this to see how it affects the model.\n",
    "\n",
    "> **TODO:** Complete the implementation for the `build_dict()` method below. Note that even though the vocab_size is set to `5000`, we only want to construct a mapping for the most frequently appearing `4998` words. This is because we want to reserve the special labels `0` for 'no word' and `1` for 'infrequent word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strang', 'enough', 'movi', 'never', 'made', 'big', 'screen', 'denmark', 'wait', 'video', 'releas', 'expect', 'high', 'way', 'disappoint', 'alway', 'ang', 'lee', 'fantast', 'act', 'intellig', 'thrill', 'plot', 'guess', 'right', 'till', 'end', 'superb', 'film', 'along', 'unforgiven', 'easili', 'one', 'two', 'best', 'western', '90', 'peopl', 'expect', 'someth', 'along', 'line', 'mel', 'gibson', 'patriot', 'corni', 'braveheart', 'accept', 'sourli', 'disappoint', 'other', 'appreci', 'mention', 'qualiti', 'fantast', 'time', 'watch', '9', '10']\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    \n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "    word_count = collections.OrderedDict(word_count)\n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    sorted_words = []\n",
    "    \n",
    "    for k in word_count:\n",
    "        sorted_words.append(k)\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the five most frequently appearing (tokenized) words in the training set? Does it makes sense that these words appear frequently in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** 'strang', 'enough', 'movi', 'never', 'made'.\n",
    "I would say that these words make sense, other than strange.  Not sure why that would be the most often used word in a set of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strang 2\n",
      "enough 3\n",
      "movi 4\n",
      "never 5\n",
      "made 6\n",
      "big 7\n",
      "screen 8\n",
      "denmark 9\n",
      "wait 10\n",
      "video 11\n",
      "releas 12\n",
      "expect 13\n",
      "high 14\n",
      "way 15\n",
      "disappoint 16\n",
      "alway 17\n",
      "ang 18\n",
      "lee 19\n",
      "fantast 20\n",
      "act 21\n",
      "intellig 22\n",
      "thrill 23\n",
      "plot 24\n",
      "guess 25\n",
      "right 26\n",
      "till 27\n",
      "end 28\n",
      "superb 29\n",
      "film 30\n",
      "along 31\n",
      "unforgiven 32\n",
      "easili 33\n",
      "one 34\n",
      "two 35\n",
      "best 36\n",
      "western 37\n",
      "90 38\n",
      "peopl 39\n",
      "someth 40\n",
      "line 41\n",
      "mel 42\n",
      "gibson 43\n",
      "patriot 44\n",
      "corni 45\n",
      "braveheart 46\n",
      "accept 47\n",
      "sourli 48\n",
      "other 49\n",
      "appreci 50\n",
      "mention 51\n",
      "qualiti 52\n",
      "time 53\n",
      "watch 54\n",
      "9 55\n",
      "10 56\n",
      "lion 57\n",
      "king 58\n",
      "disney 59\n",
      "version 60\n",
      "hamlet 61\n",
      "3 62\n",
      "hakuna 63\n",
      "matata 64\n",
      "guildenstern 65\n",
      "rosencrantz 66\n",
      "dead 67\n",
      "like 68\n",
      "tom 69\n",
      "stoppard 70\n",
      "beguil 71\n",
      "get 72\n",
      "view 73\n",
      "action 74\n",
      "point 75\n",
      "minor 76\n",
      "charact 77\n",
      "origin 78\n",
      "timon 79\n",
      "meerkat 80\n",
      "penchant 81\n",
      "break 82\n",
      "song 83\n",
      "drop 84\n",
      "hat 85\n",
      "pumbaa 86\n",
      "warthog 87\n",
      "flatul 88\n",
      "issu 89\n",
      "follow 90\n",
      "stori 91\n",
      "rather 92\n",
      "simba 93\n",
      "see 94\n",
      "anim 95\n",
      "bow 96\n",
      "present 97\n",
      "pride 98\n",
      "rock 99\n",
      "find 100\n",
      "decid 101\n",
      "back 102\n",
      "oust 103\n",
      "scar 104\n",
      "dealt 105\n",
      "hyena 106\n",
      "nathan 107\n",
      "lane 108\n",
      "joke 109\n",
      "abli 110\n",
      "support 111\n",
      "erni 112\n",
      "sabella 113\n",
      "also 114\n",
      "good 115\n",
      "hear 116\n",
      "matthew 117\n",
      "broderick 118\n",
      "whoopi 119\n",
      "goldberg 120\n",
      "repris 121\n",
      "role 122\n",
      "juli 123\n",
      "kavner 124\n",
      "jerri 125\n",
      "stiller 126\n",
      "lend 127\n",
      "distinct 128\n",
      "voic 129\n",
      "new 130\n",
      "mother 131\n",
      "uncl 132\n",
      "downsid 133\n",
      "constant 134\n",
      "stop 135\n",
      "start 136\n",
      "rewind 137\n",
      "fast 138\n",
      "forward 139\n",
      "devic 140\n",
      "help 141\n",
      "progress 142\n",
      "said 143\n",
      "brilliant 144\n",
      "zoom 145\n",
      "near 146\n",
      "begin 147\n",
      "laugh 148\n",
      "third 149\n",
      "seri 150\n",
      "worth 151\n",
      "hot 152\n",
      "tub 153\n",
      "scene 154\n",
      "still 155\n",
      "funni 156\n",
      "despit 157\n",
      "littl 158\n",
      "bit 159\n",
      "predict 160\n",
      "surfac 161\n",
      "would 162\n",
      "appear 163\n",
      "deal 164\n",
      "psycholog 165\n",
      "process 166\n",
      "call 167\n",
      "individu 168\n",
      "becom 169\n",
      "true 170\n",
      "self 171\n",
      "embrac 172\n",
      "dark 173\n",
      "side 174\n",
      "human 175\n",
      "natur 176\n",
      "thu 177\n",
      "darkl 178\n",
      "classic 179\n",
      "shadowi 180\n",
      "devilish 181\n",
      "creatur 182\n",
      "desper 183\n",
      "seek 184\n",
      "compani 185\n",
      "recognit 186\n",
      "men 187\n",
      "revolv 188\n",
      "around 189\n",
      "variou 190\n",
      "need 191\n",
      "handl 192\n",
      "less 193\n",
      "success 194\n",
      "howev 195\n",
      "dig 196\n",
      "deeper 197\n",
      "actual 198\n",
      "relat 199\n",
      "car 200\n",
      "person 201\n",
      "open 202\n",
      "main 203\n",
      "male 204\n",
      "mechan 205\n",
      "fallen 206\n",
      "grace 207\n",
      "collect 208\n",
      "piec 209\n",
      "wreck 210\n",
      "daughter 211\n",
      "nearli 212\n",
      "smash 213\n",
      "girl 214\n",
      "lesson 215\n",
      "1 216\n",
      "embodi 217\n",
      "immort 218\n",
      "soul 219\n",
      "steal 220\n",
      "ident 221\n",
      "grave 222\n",
      "robberi 223\n",
      "wick 224\n",
      "disturb 225\n",
      "must 226\n",
      "punish 227\n",
      "anoth 228\n",
      "rubin 229\n",
      "buy 230\n",
      "intend 231\n",
      "repair 232\n",
      "sell 233\n",
      "lost 234\n",
      "found 235\n",
      "famou 236\n",
      "race 237\n",
      "warn 238\n",
      "salesman 239\n",
      "2 240\n",
      "uniqu 241\n",
      "cannot 242\n",
      "alter 243\n",
      "replac 244\n",
      "addit 245\n",
      "twist 246\n",
      "hidden 247\n",
      "think 248\n",
      "junk 249\n",
      "eventu 250\n",
      "turn 251\n",
      "project 252\n",
      "greed 253\n",
      "profit 254\n",
      "thou 255\n",
      "shalt 256\n",
      "treat 257\n",
      "thi 258\n",
      "mean 259\n",
      "introduc 260\n",
      "importantli 261\n",
      "assess 262\n",
      "base 263\n",
      "outer 264\n",
      "thorough 265\n",
      "look 266\n",
      "insid 267\n",
      "engin 268\n",
      "room 269\n",
      "4 270\n",
      "judg 271\n",
      "realli 272\n",
      "count 273\n",
      "store 274\n",
      "keep 275\n",
      "mind 276\n",
      "man 277\n",
      "tri 278\n",
      "fake 279\n",
      "collector 280\n",
      "underlin 281\n",
      "import 282\n",
      "numer 283\n",
      "exampl 284\n",
      "theme 285\n",
      "tire 286\n",
      "bother 287\n",
      "cite 288\n",
      "remain 289\n",
      "say 290\n",
      "fun 291\n",
      "absolut 292\n",
      "noth 293\n",
      "els 294\n",
      "devote 295\n",
      "take 296\n",
      "patienc 297\n",
      "david 298\n",
      "lynch 299\n",
      "eccentr 300\n",
      "chang 301\n",
      "life 302\n",
      "affirm 303\n",
      "chronicl 304\n",
      "alvin 305\n",
      "straight 306\n",
      "journey 307\n",
      "stick 308\n",
      "though 309\n",
      "move 310\n",
      "slow 311\n",
      "john 312\n",
      "deer 313\n",
      "meet 314\n",
      "kind 315\n",
      "stranger 316\n",
      "pilgrimag 317\n",
      "learn 318\n",
      "much 319\n",
      "isol 320\n",
      "age 321\n",
      "pain 322\n",
      "regret 323\n",
      "secret 324\n",
      "ultim 325\n",
      "power 326\n",
      "famili 327\n",
      "reconcili 328\n",
      "richard 329\n",
      "farnsworth 330\n",
      "cap 331\n",
      "career 332\n",
      "year 333\n",
      "genuin 334\n",
      "perform 335\n",
      "sad 336\n",
      "poetic 337\n",
      "flinti 338\n",
      "care 339\n",
      "sissi 340\n",
      "spacek 341\n",
      "match 342\n",
      "rose 343\n",
      "pine 344\n",
      "privat 345\n",
      "loss 346\n",
      "dad 347\n",
      "rare 348\n",
      "modern 349\n",
      "preach 350\n",
      "posit 351\n",
      "sleepless 352\n",
      "night 353\n",
      "switch 354\n",
      "channel 355\n",
      "embarrass 356\n",
      "remot 357\n",
      "control 358\n",
      "kari 359\n",
      "wuhrer 360\n",
      "salin 361\n",
      "wonder 362\n",
      "gone 363\n",
      "nowher 364\n",
      "keith 365\n",
      "pathet 366\n",
      "anyway 367\n",
      "part 368\n",
      "nerdi 369\n",
      "colleg 370\n",
      "kid 371\n",
      "bring 372\n",
      "home 373\n",
      "dominatrix 374\n",
      "ish 375\n",
      "comic 376\n",
      "book 377\n",
      "cheap 378\n",
      "porn 379\n",
      "anal 380\n",
      "retent 381\n",
      "kiss 382\n",
      "father 383\n",
      "oh 384\n",
      "tongu 385\n",
      "great 386\n",
      "well 387\n",
      "cours 388\n",
      "bitch 389\n",
      "hell 390\n",
      "helluva 391\n",
      "past 392\n",
      "swat 393\n",
      "team 394\n",
      "upstair 395\n",
      "ye 396\n",
      "surround 397\n",
      "blow 398\n",
      "brain 399\n",
      "ahahhahahaha 400\n",
      "tv 401\n",
      "set 402\n",
      "world 403\n",
      "depress 404\n",
      "era 405\n",
      "pragu 406\n",
      "ambiti 407\n",
      "clerk 408\n",
      "fall 409\n",
      "love 410\n",
      "mysteri 411\n",
      "woman 412\n",
      "exchang 413\n",
      "romant 414\n",
      "letter 415\n",
      "discov 416\n",
      "none 417\n",
      "sale 418\n",
      "shop 419\n",
      "seem 420\n",
      "constantli 421\n",
      "bicker 422\n",
      "colleagu 423\n",
      "add 424\n",
      "owner 425\n",
      "convinc 426\n",
      "favorit 427\n",
      "employe 428\n",
      "stewart 429\n",
      "affair 430\n",
      "wife 431\n",
      "leav 432\n",
      "briefli 433\n",
      "fire 434\n",
      "admiss 435\n",
      "happi 436\n",
      "inevit 437\n",
      "although 438\n",
      "date 439\n",
      "refer 440\n",
      "poverti 441\n",
      "consid 442\n",
      "use 443\n",
      "indic 444\n",
      "mani 445\n",
      "small 446\n",
      "object 447\n",
      "pleasur 448\n",
      "music 449\n",
      "cigar 450\n",
      "box 451\n",
      "reach 452\n",
      "common 453\n",
      "enjoy 454\n",
      "effect 455\n",
      "credibl 456\n",
      "1990 457\n",
      "make 458\n",
      "got 459\n",
      "mail 460\n",
      "star 461\n",
      "hank 462\n",
      "meg 463\n",
      "ryan 464\n",
      "odd 465\n",
      "chain 466\n",
      "event 467\n",
      "unbeliev 468\n",
      "viewer 469\n",
      "grossli 470\n",
      "offend 471\n",
      "corner 472\n",
      "innoc 473\n",
      "stroll 474\n",
      "memori 475\n",
      "complic 476\n",
      "hectic 477\n",
      "place 478\n",
      "known 479\n",
      "novelist 480\n",
      "utopia 481\n",
      "lover 482\n",
      "comedi 483\n",
      "pictur 484\n",
      "long 485\n",
      "ago 486\n",
      "aprox 487\n",
      "rememb 488\n",
      "day 489\n",
      "second 490\n",
      "pleasant 491\n",
      "illog 492\n",
      "unreason 493\n",
      "sure 494\n",
      "stretch 495\n",
      "actress 496\n",
      "play 497\n",
      "women 498\n",
      "gotten 499\n",
      "away 500\n",
      "first 501\n",
      "2nd 502\n",
      "note 503\n",
      "tt 504\n",
      "realis 505\n",
      "even 506\n",
      "came 507\n",
      "load 508\n",
      "money 509\n",
      "court 510\n",
      "ask 511\n",
      "logic 512\n",
      "answer 513\n",
      "ding 514\n",
      "crook 515\n",
      "horribl 516\n",
      "stereotyp 517\n",
      "black 518\n",
      "basic 519\n",
      "beat 520\n",
      "system 521\n",
      "obtain 522\n",
      "larg 523\n",
      "amount 524\n",
      "resourc 525\n",
      "sympathi 526\n",
      "b 527\n",
      "c 528\n",
      "die 529\n",
      "bar 530\n",
      "jada 531\n",
      "stupid 532\n",
      "deserv 533\n",
      "unless 534\n",
      "shallow 535\n",
      "believ 536\n",
      "rate 537\n",
      "5 538\n",
      "excel 539\n",
      "write 540\n",
      "wild 541\n",
      "cast 542\n",
      "tech 543\n",
      "poor 544\n",
      "obvious 545\n",
      "low 546\n",
      "budget 547\n",
      "cut 548\n",
      "neg 549\n",
      "output 550\n",
      "case 551\n",
      "invent 552\n",
      "seen 553\n",
      "late 554\n",
      "screenwrit 555\n",
      "particular 556\n",
      "fine 557\n",
      "sort 558\n",
      "negat 559\n",
      "tast 560\n",
      "ani 561\n",
      "difranco 562\n",
      "guitarist 563\n",
      "songwrit 564\n",
      "admit 565\n",
      "redeem 566\n",
      "accur 567\n",
      "portray 568\n",
      "school 569\n",
      "student 570\n",
      "face 571\n",
      "mayb 572\n",
      "everyth 573\n",
      "far 574\n",
      "fetch 575\n",
      "attempt 576\n",
      "nake 577\n",
      "gun 578\n",
      "esqu 579\n",
      "space 580\n",
      "actor 581\n",
      "hold 582\n",
      "laughter 583\n",
      "whoever 584\n",
      "wrote 585\n",
      "script 586\n",
      "cram 587\n",
      "controversi 588\n",
      "togeth 589\n",
      "almost 590\n",
      "fulli 591\n",
      "develop 592\n",
      "especi 593\n",
      "impregn 594\n",
      "teacher 595\n",
      "throughout 596\n",
      "entir 597\n",
      "insult 598\n",
      "humor 599\n",
      "satir 600\n",
      "anyth 601\n",
      "roll 602\n",
      "eye 603\n",
      "17th 604\n",
      "centuri 605\n",
      "japan 606\n",
      "live 607\n",
      "samurai 608\n",
      "standard 609\n",
      "name 610\n",
      "mayeda 611\n",
      "sent 612\n",
      "epic 613\n",
      "across 614\n",
      "acquir 615\n",
      "000 616\n",
      "muscat 617\n",
      "spain 618\n",
      "whilst 619\n",
      "sea 620\n",
      "violent 621\n",
      "storm 622\n",
      "swallow 623\n",
      "preciou 624\n",
      "gold 625\n",
      "weapon 626\n",
      "battl 627\n",
      "surviv 628\n",
      "secur 629\n",
      "fate 630\n",
      "belov 631\n",
      "shogun 632\n",
      "multi 633\n",
      "million 634\n",
      "dollar 635\n",
      "adventur 636\n",
      "three 637\n",
      "contin 638\n",
      "cinema 639\n",
      "legend 640\n",
      "sho 641\n",
      "kosugi 642\n",
      "tenchu 643\n",
      "stealth 644\n",
      "assassin 645\n",
      "christoph 646\n",
      "war 647\n",
      "lord 648\n",
      "ring 649\n",
      "trilog 650\n",
      "rhi 651\n",
      "davi 652\n",
      "indiana 653\n",
      "jone 654\n",
      "toshiro 655\n",
      "milfun 656\n",
      "seven 657\n",
      "throne 658\n",
      "blood 659\n",
      "kabuto 660\n",
      "masterpiec 661\n",
      "osaka 662\n",
      "win 663\n",
      "stunt 664\n",
      "bought 665\n",
      "bob 666\n",
      "ivi 667\n",
      "excit 668\n",
      "sequenc 669\n",
      "eastern 670\n",
      "armi 671\n",
      "attack 672\n",
      "ship 673\n",
      "carri 674\n",
      "priceless 675\n",
      "jewel 676\n",
      "final 677\n",
      "confront 678\n",
      "essex 679\n",
      "score 680\n",
      "fame 681\n",
      "compos 682\n",
      "scott 683\n",
      "director 684\n",
      "kusugi 685\n",
      "nomin 686\n",
      "oscar 687\n",
      "emot 688\n",
      "strife 689\n",
      "death 690\n",
      "search 691\n",
      "griev 692\n",
      "extrem 693\n",
      "highli 694\n",
      "recommend 695\n",
      "cinemat 696\n",
      "pleas 697\n",
      "opposit 698\n",
      "question 699\n",
      "70 700\n",
      "80 701\n",
      "kong 702\n",
      "martial 703\n",
      "art 704\n",
      "bare 705\n",
      "function 706\n",
      "usual 707\n",
      "plan 708\n",
      "fight 709\n",
      "scrape 710\n",
      "pretti 711\n",
      "thin 712\n",
      "fill 713\n",
      "gap 714\n",
      "node 715\n",
      "porno 716\n",
      "sever 717\n",
      "thing 718\n",
      "overtli 719\n",
      "direct 720\n",
      "choreographi 721\n",
      "confin 722\n",
      "combat 723\n",
      "style 724\n",
      "chines 725\n",
      "circu 726\n",
      "acrobat 727\n",
      "comed 728\n",
      "theater 729\n",
      "work 730\n",
      "languag 731\n",
      "camera 732\n",
      "impress 733\n",
      "techniqu 734\n",
      "today 735\n",
      "measur 736\n",
      "speed 737\n",
      "real 738\n",
      "motion 739\n",
      "unfilt 740\n",
      "filter 741\n",
      "depict 742\n",
      "toward 743\n",
      "futur 744\n",
      "least 745\n",
      "overt 746\n",
      "unexpect 747\n",
      "construct 748\n",
      "winner 749\n",
      "contest 750\n",
      "determin 751\n",
      "unfold 752\n",
      "master 753\n",
      "writer 754\n",
      "game 755\n",
      "lead 756\n",
      "know 757\n",
      "situat 758\n",
      "figur 759\n",
      "tragedi 760\n",
      "noir 761\n",
      "arc 762\n",
      "tend 763\n",
      "ironi 764\n",
      "la 765\n",
      "sting 766\n",
      "clever 767\n",
      "stuff 768\n",
      "genr 769\n",
      "five 770\n",
      "venom 771\n",
      "idea 772\n",
      "templat 773\n",
      "tarantino 774\n",
      "deadli 775\n",
      "viper 776\n",
      "kill 777\n",
      "bill 778\n",
      "volum 779\n",
      "recent 780\n",
      "post 781\n",
      "claim 782\n",
      "militari 783\n",
      "background 784\n",
      "contain 785\n",
      "comment 786\n",
      "valuabl 787\n",
      "mantra 788\n",
      "polit 789\n",
      "correct 790\n",
      "histori 791\n",
      "biolog 792\n",
      "show 793\n",
      "societi 794\n",
      "manag 795\n",
      "recov 796\n",
      "heavi 797\n",
      "popul 798\n",
      "sometim 799\n",
      "astonish 800\n",
      "germani 801\n",
      "readi 802\n",
      "1939 803\n",
      "1914 804\n",
      "1918 805\n",
      "south 806\n",
      "america 807\n",
      "tripl 808\n",
      "allianc 809\n",
      "1865 810\n",
      "paraguay 811\n",
      "took 812\n",
      "neighbor 813\n",
      "countri 814\n",
      "virtual 815\n",
      "wipe 816\n",
      "fought 817\n",
      "stalem 818\n",
      "1932 819\n",
      "chaco 820\n",
      "larger 821\n",
      "bolivia 822\n",
      "ever 823\n",
      "could 824\n",
      "femal 825\n",
      "nation 826\n",
      "stake 827\n",
      "israel 828\n",
      "1948 829\n",
      "sinc 830\n",
      "co 831\n",
      "unit 832\n",
      "defens 833\n",
      "forc 834\n",
      "popular 835\n",
      "imag 836\n",
      "isra 837\n",
      "soldier 838\n",
      "g 839\n",
      "jane 840\n",
      "hollywood 841\n",
      "fluff 842\n",
      "lot 843\n",
      "lack 844\n",
      "formula 845\n",
      "conflict 846\n",
      "resolv 847\n",
      "rich 848\n",
      "list 849\n",
      "provid 850\n",
      "thought 851\n",
      "provok 852\n",
      "entertain 853\n",
      "cinematographi 854\n",
      "given 855\n",
      "urban 856\n",
      "innov 857\n",
      "normal 858\n",
      "risk 859\n",
      "differ 860\n",
      "purpos 861\n",
      "avoid 862\n",
      "someon 863\n",
      "interpret 864\n",
      "earth 865\n",
      "sweet 866\n",
      "similar 867\n",
      "concept 868\n",
      "piti 869\n",
      "flesh 870\n",
      "want 871\n",
      "dench 872\n",
      "dukaki 873\n",
      "lain 874\n",
      "alon 875\n",
      "substanc 876\n",
      "lesser 877\n",
      "moment 878\n",
      "tape 879\n",
      "continu 880\n",
      "hope 881\n",
      "clue 882\n",
      "suggest 883\n",
      "sequel 884\n",
      "top 885\n",
      "drawer 886\n",
      "argument 887\n",
      "barbara 888\n",
      "steel 889\n",
      "sucker 890\n",
      "haunt 891\n",
      "hous 892\n",
      "hill 893\n",
      "feel 894\n",
      "castl 895\n",
      "alley 896\n",
      "boy 897\n",
      "french 898\n",
      "italian 899\n",
      "coproduct 900\n",
      "perhap 901\n",
      "horror 902\n",
      "sunday 903\n",
      "nevertheless 904\n",
      "atmospher 905\n",
      "chill 906\n",
      "entri 907\n",
      "spook 908\n",
      "white 909\n",
      "convey 910\n",
      "creepi 911\n",
      "miasma 912\n",
      "concern 913\n",
      "journalist 914\n",
      "bet 915\n",
      "blackwood 916\n",
      "author 917\n",
      "edgar 918\n",
      "allen 919\n",
      "poe 920\n",
      "spend 921\n",
      "saint 922\n",
      "spirit 923\n",
      "reenact 924\n",
      "inde 925\n",
      "send 926\n",
      "shiver 927\n",
      "spine 928\n",
      "uncut 929\n",
      "dvd 930\n",
      "thank 931\n",
      "folk 932\n",
      "synaps 933\n",
      "featur 934\n",
      "surpris 935\n",
      "topless 936\n",
      "mild 937\n",
      "lesbian 938\n",
      "otherworldli 939\n",
      "beauti 940\n",
      "put 941\n",
      "advantag 942\n",
      "sympathet 943\n",
      "spectr 944\n",
      "mere 945\n",
      "presenc 946\n",
      "ghost 947\n",
      "truli 948\n",
      "memor 949\n",
      "queen 950\n",
      "favourit 951\n",
      "walk 952\n",
      "clich 953\n",
      "includ 954\n",
      "violenc 955\n",
      "mafia 956\n",
      "sex 957\n",
      "gambl 958\n",
      "drug 959\n",
      "etc 960\n",
      "alreadi 961\n",
      "combin 962\n",
      "superce 963\n",
      "fit 964\n",
      "vintag 965\n",
      "type 966\n",
      "that 967\n",
      "gonna 968\n",
      "tough 969\n",
      "compet 970\n",
      "stand 971\n",
      "jame 972\n",
      "gandolfini 973\n",
      "toni 974\n",
      "soprano 975\n",
      "edi 976\n",
      "falco 977\n",
      "carmela 978\n",
      "word 979\n",
      "express 980\n",
      "yet 981\n",
      "go 982\n",
      "everi 983\n",
      "brillianc 984\n",
      "grip 985\n",
      "abl 986\n",
      "seinfeld 987\n",
      "finish 988\n",
      "definit 989\n",
      "reason 990\n",
      "strike 991\n",
      "cord 992\n",
      "glenn 993\n",
      "winch 994\n",
      "dislik 995\n",
      "frosti 996\n",
      "return 997\n",
      "absurd 998\n",
      "poorli 999\n",
      "written 1000\n",
      "bad 1001\n",
      "done 1002\n",
      "worst 1003\n",
      "unalik 1004\n",
      "snowman 1005\n",
      "drew 1006\n",
      "winterwond 1007\n",
      "land 1008\n",
      "heartwarm 1009\n",
      "drama 1010\n",
      "alik 1011\n",
      "60 1012\n",
      "talk 1013\n",
      "window 1014\n",
      "hum 1015\n",
      "whistl 1016\n",
      "whatev 1017\n",
      "badli 1018\n",
      "saw 1019\n",
      "cover 1020\n",
      "librari 1021\n",
      "crappi 1022\n",
      "pick 1023\n",
      "touch 1024\n",
      "hassl 1025\n",
      "terribl 1026\n",
      "dull 1027\n",
      "fell 1028\n",
      "asleep 1029\n",
      "whole 1030\n",
      "sleep 1031\n",
      "overal 1032\n",
      "boringoveral 1033\n",
      "grade 1034\n",
      "cthi 1035\n",
      "holiday 1036\n",
      "special 1037\n",
      "rip 1038\n",
      "4th 1039\n",
      "ok 1040\n",
      "hate 1041\n",
      "sister 1042\n",
      "minut 1043\n",
      "half 1044\n",
      "agre 1045\n",
      "3rd 1046\n",
      "plain 1047\n",
      "aw 1048\n",
      "influenc 1049\n",
      "frazetta 1050\n",
      "bakshi 1051\n",
      "obviou 1052\n",
      "anyon 1053\n",
      "conan 1054\n",
      "barbarian 1055\n",
      "dungeon 1056\n",
      "dragon 1057\n",
      "evil 1058\n",
      "clear 1059\n",
      "may 1060\n",
      "hero 1061\n",
      "neutral 1062\n",
      "often 1063\n",
      "fantasi 1064\n",
      "elv 1065\n",
      "skin 1066\n",
      "blond 1067\n",
      "hair 1068\n",
      "goblin 1069\n",
      "orc 1070\n",
      "familiar 1071\n",
      "tolkien 1072\n",
      "racist 1073\n",
      "young 1074\n",
      "children 1075\n",
      "due 1076\n",
      "sexual 1077\n",
      "innuendo 1078\n",
      "gem 1079\n",
      "soon 1080\n",
      "lucki 1081\n",
      "vh 1082\n",
      "copi 1083\n",
      "condit 1084\n",
      "pauli 1085\n",
      "heart 1086\n",
      "witti 1087\n",
      "comer 1088\n",
      "fan 1089\n",
      "2004 1090\n",
      "order 1091\n",
      "widescreen 1092\n",
      "seller 1093\n",
      "ebay 1094\n",
      "son 1095\n",
      "bird 1096\n",
      "knew 1097\n",
      "phrase 1098\n",
      "probabl 1099\n",
      "gena 1100\n",
      "rowland 1101\n",
      "cheech 1102\n",
      "marin 1103\n",
      "ignacio 1104\n",
      "shalhoub 1105\n",
      "misha 1106\n",
      "root 1107\n",
      "wrong 1108\n",
      "rent 1109\n",
      "went 1110\n",
      "prescreen 1111\n",
      "shock 1112\n",
      "cheesi 1113\n",
      "thriller 1114\n",
      "pedophilia 1115\n",
      "satan 1116\n",
      "worship 1117\n",
      "undercov 1118\n",
      "cop 1119\n",
      "religion 1120\n",
      "mess 1121\n",
      "washboard 1122\n",
      "jesu 1123\n",
      "dude 1124\n",
      "alright 1125\n",
      "apart 1126\n",
      "admir 1127\n",
      "effort 1128\n",
      "slightli 1129\n",
      "fail 1130\n",
      "christian 1131\n",
      "gospel 1132\n",
      "stock 1133\n",
      "kiddish 1134\n",
      "gosh 1135\n",
      "toooo 1136\n",
      "dramat 1137\n",
      "okay 1138\n",
      "felt 1139\n",
      "premier 1140\n",
      "rant 1141\n",
      "bottom 1142\n",
      "wast 1143\n",
      "hour 1144\n",
      "fifti 1145\n",
      "crap 1146\n",
      "trash 1147\n",
      "resist 1148\n",
      "freddi 1149\n",
      "vs 1150\n",
      "jason 1151\n",
      "disgrac 1152\n",
      "sit 1153\n",
      "better 1154\n",
      "camcord 1155\n",
      "fact 1156\n",
      "zombi 1157\n",
      "left 1158\n",
      "neck 1159\n",
      "hand 1160\n",
      "weird 1161\n",
      "flashback 1162\n",
      "sens 1163\n",
      "broke 1164\n",
      "tooth 1165\n",
      "vampir 1166\n",
      "respons 1167\n",
      "jessi 1168\n",
      "duke 1169\n",
      "cross 1170\n",
      "santa 1171\n",
      "theatr 1172\n",
      "foreign 1173\n",
      "festiv 1174\n",
      "intrigu 1175\n",
      "fascin 1176\n",
      "sensit 1177\n",
      "bi 1178\n",
      "artist 1179\n",
      "european 1180\n",
      "dutch 1181\n",
      "hardli 1182\n",
      "overwork 1183\n",
      "mad 1184\n",
      "religi 1185\n",
      "orient 1186\n",
      "persecut 1187\n",
      "complex 1188\n",
      "guy 1189\n",
      "freeload 1190\n",
      "suspect 1191\n",
      "nightmar 1192\n",
      "spiderwoman 1193\n",
      "guilt 1194\n",
      "kick 1195\n",
      "ignor 1196\n",
      "advic 1197\n",
      "bed 1198\n",
      "outrag 1199\n",
      "sum 1200\n",
      "taxfre 1201\n",
      "loot 1202\n",
      "lie 1203\n",
      "trip 1204\n",
      "pay 1205\n",
      "henc 1206\n",
      "hustl 1207\n",
      "salon 1208\n",
      "nerv 1209\n",
      "complain 1210\n",
      "alot 1211\n",
      "seriou 1212\n",
      "jan 1213\n",
      "de 1214\n",
      "bont 1215\n",
      "paul 1216\n",
      "verhoeven 1217\n",
      "earlier 1218\n",
      "hmmm 1219\n",
      "download 1220\n",
      "sourc 1221\n",
      "ruin 1222\n",
      "demo 1223\n",
      "reel 1224\n",
      "lip 1225\n",
      "sync 1226\n",
      "understand 1227\n",
      "realiz 1228\n",
      "creator 1229\n",
      "short 1230\n",
      "twice 1231\n",
      "taken 1232\n",
      "hire 1233\n",
      "everyon 1234\n",
      "blender 1235\n",
      "showcas 1236\n",
      "softwar 1237\n",
      "render 1238\n",
      "capabl 1239\n",
      "sorri 1240\n",
      "tell 1241\n",
      "julia 1242\n",
      "robert 1243\n",
      "concert 1244\n",
      "shake 1245\n",
      "cotton 1246\n",
      "wool 1247\n",
      "persona 1248\n",
      "spuriou 1249\n",
      "spousal 1250\n",
      "abus 1251\n",
      "hard 1252\n",
      "imagin 1253\n",
      "erin 1254\n",
      "brokovich 1255\n",
      "rubbish 1256\n",
      "bone 1257\n",
      "vehicl 1258\n",
      "unfortun 1259\n",
      "lacklustr 1260\n",
      "mark 1261\n",
      "cabl 1262\n",
      "offer 1263\n",
      "insight 1264\n",
      "domest 1265\n",
      "non 1266\n",
      "sketch 1267\n",
      "unsatisfi 1268\n",
      "without 1269\n",
      "titl 1270\n",
      "total 1271\n",
      "flop 1272\n",
      "deservedli 1273\n",
      "moral 1274\n",
      "occasion 1275\n",
      "gentl 1276\n",
      "bitter 1277\n",
      "occur 1278\n",
      "light 1279\n",
      "soften 1280\n",
      "sarcasm 1281\n",
      "flow 1282\n",
      "sudden 1283\n",
      "halt 1284\n",
      "marvel 1285\n",
      "shine 1286\n",
      "perfectli 1287\n",
      "jean 1288\n",
      "loui 1289\n",
      "trintign 1290\n",
      "exquisit 1291\n",
      "romi 1292\n",
      "schneider 1293\n",
      "pearl 1294\n",
      "perfect 1295\n",
      "glow 1296\n",
      "miss 1297\n",
      "sentinel 1298\n",
      "industri 1299\n",
      "scari 1300\n",
      "alison 1301\n",
      "parker 1302\n",
      "model 1303\n",
      "lawyer 1304\n",
      "michael 1305\n",
      "lerman 1306\n",
      "friend 1307\n",
      "jennif 1308\n",
      "old 1309\n",
      "problem 1310\n",
      "suddenli 1311\n",
      "health 1312\n",
      "faint 1313\n",
      "frequenc 1314\n",
      "illus 1315\n",
      "exorcist 1316\n",
      "scariest 1317\n",
      "occupi 1318\n",
      "priest 1319\n",
      "froze 1320\n",
      "blind 1321\n",
      "monster 1322\n",
      "purchas 1323\n",
      "complet 1324\n",
      "american 1325\n",
      "gothic 1326\n",
      "grate 1327\n",
      "episod 1328\n",
      "televis 1329\n",
      "gari 1330\n",
      "cole 1331\n",
      "sexi 1332\n",
      "luca 1333\n",
      "buck 1334\n",
      "caleb 1335\n",
      "player 1336\n",
      "brenda 1337\n",
      "bakk 1338\n",
      "selena 1339\n",
      "coomb 1340\n",
      "talent 1341\n",
      "chemistri 1342\n",
      "shame 1343\n",
      "screw 1344\n",
      "network 1345\n",
      "collus 1346\n",
      "burgeon 1347\n",
      "group 1348\n",
      "censor 1349\n",
      "design 1350\n",
      "adult 1351\n",
      "mixtur 1352\n",
      "farc 1353\n",
      "romanc 1354\n",
      "character 1355\n",
      "geniu 1356\n",
      "level 1357\n",
      "tremend 1358\n",
      "lust 1359\n",
      "devil 1360\n",
      "sheriff 1361\n",
      "lusciou 1362\n",
      "angel 1363\n",
      "fashion 1364\n",
      "alic 1365\n",
      "term 1366\n",
      "yeah 1367\n",
      "asid 1368\n",
      "graini 1369\n",
      "42 1370\n",
      "tweedl 1371\n",
      "dee 1372\n",
      "dum 1373\n",
      "mudler 1374\n",
      "sculli 1375\n",
      "serisouli 1376\n",
      "staff 1377\n",
      "mr 1378\n",
      "carrol 1379\n",
      "somebodi 1380\n",
      "tale 1381\n",
      "satisfi 1382\n",
      "element 1383\n",
      "dan 1384\n",
      "relationship 1385\n",
      "steve 1386\n",
      "carel 1387\n",
      "mari 1388\n",
      "juliett 1389\n",
      "binoch 1390\n",
      "realist 1391\n",
      "interest 1392\n",
      "decis 1393\n",
      "except 1394\n",
      "attract 1395\n",
      "appropri 1396\n",
      "jessica 1397\n",
      "alba 1398\n",
      "scarlett 1399\n",
      "johansson 1400\n",
      "disast 1401\n",
      "aspect 1402\n",
      "matur 1403\n",
      "pillar 1404\n",
      "frat 1405\n",
      "pack 1406\n",
      "dumb 1407\n",
      "chick 1408\n",
      "chauvinist 1409\n",
      "sleazi 1410\n",
      "wherea 1411\n",
      "incorpor 1412\n",
      "audienc 1413\n",
      "involv 1414\n",
      "rebelli 1415\n",
      "middl 1416\n",
      "brittani 1417\n",
      "robertson 1418\n",
      "difficult 1419\n",
      "choic 1420\n",
      "either 1421\n",
      "unrealist 1422\n",
      "annoy 1423\n",
      "flaw 1424\n",
      "escap 1425\n",
      "redund 1426\n",
      "offens 1427\n",
      "simpli 1428\n",
      "goe 1429\n",
      "scienc 1430\n",
      "fiction 1431\n",
      "cool 1432\n",
      "cube 1433\n",
      "utter 1434\n",
      "bullsh 1435\n",
      "milk 1436\n",
      "somewhat 1437\n",
      "translat 1438\n",
      "core 1439\n",
      "introduct 1440\n",
      "android 1441\n",
      "behind 1442\n",
      "gave 1443\n",
      "backstab 1444\n",
      "kept 1445\n",
      "steril 1446\n",
      "watchabl 1447\n",
      "hypercub 1448\n",
      "compris 1449\n",
      "select 1450\n",
      "brows 1451\n",
      "blockbust 1452\n",
      "sound 1453\n",
      "quit 1454\n",
      "serious 1455\n",
      "spent 1456\n",
      "catch 1457\n",
      "garbag 1458\n",
      "wors 1459\n",
      "20 1460\n",
      "van 1461\n",
      "damm 1462\n",
      "ninja 1463\n",
      "dire 1464\n",
      "skill 1465\n",
      "sight 1466\n",
      "wooden 1467\n",
      "sculptur 1468\n",
      "2006 1469\n",
      "born 1470\n",
      "check 1471\n",
      "jaa 1472\n",
      "ong 1473\n",
      "bak 1474\n",
      "protector 1475\n",
      "proper 1476\n",
      "unintenion 1477\n",
      "humour 1478\n",
      "grim 1479\n",
      "liter 1480\n",
      "sharp 1481\n",
      "head 1482\n",
      "cgi 1483\n",
      "ink 1484\n",
      "dean 1485\n",
      "cain 1486\n",
      "super 1487\n",
      "intuit 1488\n",
      "helicopt 1489\n",
      "correctli 1490\n",
      "biggest 1491\n",
      "split 1492\n",
      "bradi 1493\n",
      "bunch 1494\n",
      "close 1495\n",
      "up 1496\n",
      "shot 1497\n",
      "push 1498\n",
      "daisi 1499\n",
      "landmark 1500\n",
      "form 1501\n",
      "homag 1502\n",
      "ameli 1503\n",
      "tim 1504\n",
      "burton 1505\n",
      "fresh 1506\n",
      "distinctli 1507\n",
      "guarante 1508\n",
      "whether 1509\n",
      "captiv 1510\n",
      "backdrop 1511\n",
      "dialogu 1512\n",
      "incred 1513\n",
      "shield 1514\n",
      "wire 1515\n",
      "exactli 1516\n",
      "comparison 1517\n",
      "addict 1518\n",
      "piemak 1519\n",
      "pie 1520\n",
      "tantalis 1521\n",
      "steven 1522\n",
      "spielberg 1523\n",
      "encount 1524\n",
      "raider 1525\n",
      "ark 1526\n",
      "e 1527\n",
      "extra 1528\n",
      "terrestri 1529\n",
      "color 1530\n",
      "purpl 1531\n",
      "period 1532\n",
      "remark 1533\n",
      "academi 1534\n",
      "voter 1535\n",
      "vote 1536\n",
      "1985 1537\n",
      "give 1538\n",
      "respect 1539\n",
      "receiv 1540\n",
      "11 1541\n",
      "unfairli 1542\n",
      "snub 1543\n",
      "singl 1544\n",
      "shut 1545\n",
      "coupl 1546\n",
      "spectacular 1547\n",
      "debut 1548\n",
      "celi 1549\n",
      "suffer 1550\n",
      "husband 1551\n",
      "fright 1552\n",
      "danni 1553\n",
      "glover 1554\n",
      "stronger 1555\n",
      "oprah 1556\n",
      "winfrey 1557\n",
      "sofia 1558\n",
      "chicago 1559\n",
      "nationwid 1560\n",
      "margaret 1561\n",
      "averi 1562\n",
      "terrif 1563\n",
      "shug 1564\n",
      "happen 1565\n",
      "mistress 1566\n",
      "rotten 1567\n",
      "20th 1568\n",
      "strong 1569\n",
      "cri 1570\n",
      "puzzl 1571\n",
      "eventulli 1572\n",
      "later 1573\n",
      "schindler 1574\n",
      "save 1575\n",
      "job 1576\n",
      "four 1577\n",
      "seventh 1578\n",
      "sign 1579\n",
      "hook 1580\n",
      "defenc 1581\n",
      "come 1582\n",
      "terrorist 1583\n",
      "grab 1584\n",
      "reader 1585\n",
      "bannon 1586\n",
      "abbi 1587\n",
      "russel 1588\n",
      "quinnn 1589\n",
      "telegraph 1590\n",
      "rest 1591\n",
      "confus 1592\n",
      "uninvolv 1593\n",
      "number 1594\n",
      "plothol 1595\n",
      "umpteen 1596\n",
      "supernatur 1597\n",
      "twister 1598\n",
      "spring 1599\n",
      "leak 1600\n",
      "wal 1601\n",
      "mart 1602\n",
      "pullman 1603\n",
      "helen 1604\n",
      "hunt 1605\n",
      "97 1606\n",
      "silli 1607\n",
      "messag 1608\n",
      "depend 1609\n",
      "studi 1610\n",
      "devon 1611\n",
      "sawa 1612\n",
      "destin 1613\n",
      "slacker 1614\n",
      "honor 1615\n",
      "latter 1616\n",
      "ulmer 1617\n",
      "portli 1618\n",
      "nice 1619\n",
      "might 1620\n",
      "pat 1621\n",
      "proft 1622\n",
      "joe 1623\n",
      "alaskey 1624\n",
      "donna 1625\n",
      "dixon 1626\n",
      "last 1627\n",
      "impact 1628\n",
      "quicki 1629\n",
      "throwaway 1630\n",
      "helm 1631\n",
      "norman 1632\n",
      "bate 1633\n",
      "anthoni 1634\n",
      "perkin 1635\n",
      "eat 1636\n",
      "raoul 1637\n",
      "amus 1638\n",
      "misfir 1639\n",
      "heard 1640\n",
      "read 1641\n",
      "articl 1642\n",
      "unknown 1643\n",
      "websit 1644\n",
      "curiou 1645\n",
      "cartoonist 1646\n",
      "illustr 1647\n",
      "william 1648\n",
      "ziggi 1649\n",
      "gift 1650\n",
      "finest 1651\n",
      "christma 1652\n",
      "frame 1653\n",
      "roger 1654\n",
      "rabbit 1655\n",
      "sixteen 1656\n",
      "highest 1657\n",
      "regard 1658\n",
      "attent 1659\n",
      "detail 1660\n",
      "creat 1661\n",
      "visual 1662\n",
      "fault 1663\n",
      "notabl 1664\n",
      "propens 1665\n",
      "schedul 1666\n",
      "testament 1667\n",
      "circumst 1668\n",
      "raggedi 1669\n",
      "ann 1670\n",
      "andi 1671\n",
      "confect 1672\n",
      "experiment 1673\n",
      "wall 1674\n",
      "within 1675\n",
      "cartoon 1676\n",
      "thousand 1677\n",
      "freakout 1678\n",
      "blower 1679\n",
      "paean 1680\n",
      "childhood 1681\n",
      "collaps 1682\n",
      "weight 1683\n",
      "limitless 1684\n",
      "ambit 1685\n",
      "aspir 1686\n",
      "bland 1687\n",
      "hallucinogen 1688\n",
      "constitut 1689\n",
      "let 1690\n",
      "wannab 1691\n",
      "toy 1692\n",
      "sing 1693\n",
      "perspect 1694\n",
      "marcella 1695\n",
      "upsid 1696\n",
      "mud 1697\n",
      "overwhelm 1698\n",
      "advisedli 1699\n",
      "huge 1700\n",
      "flat 1701\n",
      "doll 1702\n",
      "playroom 1703\n",
      "suppos 1704\n",
      "cute 1705\n",
      "lovabl 1706\n",
      "bizarr 1707\n",
      "marionett 1708\n",
      "prime 1709\n",
      "rel 1710\n",
      "compar 1711\n",
      "sneez 1712\n",
      "pirat 1713\n",
      "captain 1714\n",
      "whose 1715\n",
      "moustach 1716\n",
      "erect 1717\n",
      "groin 1718\n",
      "visibl 1719\n",
      "swell 1720\n",
      "glamor 1721\n",
      "sung 1722\n",
      "injuri 1723\n",
      "wood 1724\n",
      "scare 1725\n",
      "forev 1726\n",
      "semblanc 1727\n",
      "camel 1728\n",
      "wrinkl 1729\n",
      "knee 1730\n",
      "us 1731\n",
      "worn 1732\n",
      "fade 1733\n",
      "denim 1734\n",
      "clearli 1735\n",
      "paranoid 1736\n",
      "schizophren 1737\n",
      "hallucin 1738\n",
      "greedi 1739\n",
      "breath 1740\n",
      "belch 1741\n",
      "fart 1742\n",
      "pool 1743\n",
      "taffi 1744\n",
      "trippi 1745\n",
      "free 1746\n",
      "psychedel 1747\n",
      "experi 1748\n",
      "l 1749\n",
      "n 1750\n",
      "psychot 1751\n",
      "sir 1752\n",
      "leonard 1753\n",
      "looney 1754\n",
      "kookoo 1755\n",
      "resembl 1756\n",
      "urin 1757\n",
      "capsul 1758\n",
      "descript 1759\n",
      "rub 1760\n",
      "remind 1761\n",
      "blog 1762\n",
      "overwhelmingli 1763\n",
      "whack 1764\n",
      "winsor 1765\n",
      "mccay 1766\n",
      "nemo 1767\n",
      "rhyme 1768\n",
      "surreal 1769\n",
      "realiti 1770\n",
      "pure 1771\n",
      "indulg 1772\n",
      "unravel 1773\n",
      "ludicr 1774\n",
      "jerk 1775\n",
      "water 1776\n",
      "manner 1777\n",
      "cel 1778\n",
      "product 1779\n",
      "ran 1780\n",
      "meagr 1781\n",
      "86 1782\n",
      "length 1783\n",
      "ordeal 1784\n",
      "buff 1785\n",
      "scratch 1786\n",
      "flub 1787\n",
      "spectacularli 1788\n",
      "00 1789\n",
      "cinemax 1790\n",
      "week 1791\n",
      "award 1792\n",
      "180d 1793\n",
      "feast 1794\n",
      "spree 1795\n",
      "suck 1796\n",
      "rid 1797\n",
      "plu 1798\n",
      "strength 1799\n",
      "weak 1800\n",
      "heck 1801\n",
      "stephen 1802\n",
      "di 1803\n",
      "jimmi 1804\n",
      "morri 1805\n",
      "dream 1806\n",
      "daddi 1807\n",
      "kansa 1808\n",
      "citi 1809\n",
      "holm 1810\n",
      "chief 1811\n",
      "coolest 1812\n",
      "alli 1813\n",
      "mcbeal 1814\n",
      "decent 1815\n",
      "overr 1816\n",
      "bore 1817\n",
      "chose 1818\n",
      "outstay 1819\n",
      "welcom 1820\n",
      "review 1821\n",
      "uwe 1822\n",
      "boll 1823\n",
      "unenvi 1824\n",
      "task 1825\n",
      "arm 1826\n",
      "pull 1827\n",
      "analog 1828\n",
      "step 1829\n",
      "irredeem 1830\n",
      "gutter 1831\n",
      "teeth 1832\n",
      "extract 1833\n",
      "novacain 1834\n",
      "gener 1835\n",
      "eli 1836\n",
      "roth 1837\n",
      "cabin 1838\n",
      "fever 1839\n",
      "sheer 1840\n",
      "agon 1841\n",
      "dreck 1842\n",
      "crank 1843\n",
      "honesti 1844\n",
      "excus 1845\n",
      "valid 1846\n",
      "theatric 1847\n",
      "convolut 1848\n",
      "patchwork 1849\n",
      "potenti 1850\n",
      "unnecessari 1851\n",
      "unfunni 1852\n",
      "grotesqu 1853\n",
      "viru 1854\n",
      "dish 1855\n",
      "doubt 1856\n",
      "evid 1857\n",
      "possess 1858\n",
      "fumbl 1859\n",
      "ball 1860\n",
      "redefin 1861\n",
      "opportun 1862\n",
      "gross 1863\n",
      "unsettl 1864\n",
      "anybodi 1865\n",
      "quarter 1866\n",
      "crowd 1867\n",
      "halfway 1868\n",
      "sink 1869\n",
      "vertigo 1870\n",
      "equal 1871\n",
      "bewitch 1872\n",
      "lighter 1873\n",
      "vein 1874\n",
      "enchant 1875\n",
      "york 1876\n",
      "winter 1877\n",
      "kim 1878\n",
      "novak 1879\n",
      "witch 1880\n",
      "spell 1881\n",
      "caught 1882\n",
      "instead 1883\n",
      "sidelight 1884\n",
      "rival 1885\n",
      "janic 1886\n",
      "rule 1887\n",
      "madg 1888\n",
      "picnic 1889\n",
      "broadway 1890\n",
      "disagre 1891\n",
      "piedra 1892\n",
      "magnolia 1893\n",
      "spanish 1894\n",
      "antonia 1895\n",
      "san 1896\n",
      "juan 1897\n",
      "contrast 1898\n",
      "mistak 1899\n",
      "chanc 1900\n",
      "waterd 1901\n",
      "danc 1902\n",
      "strip 1903\n",
      "club 1904\n",
      "invalid 1905\n",
      "hospit 1906\n",
      "physic 1907\n",
      "sisabl 1908\n",
      "healthi 1909\n",
      "im 1910\n",
      "stuck 1911\n",
      "warp 1912\n",
      "earli 1913\n",
      "deterior 1914\n",
      "1920 1915\n",
      "1998 1916\n",
      "eisner 1917\n",
      "jim 1918\n",
      "month 1919\n",
      "inspir 1920\n",
      "sentiment 1921\n",
      "trick 1922\n",
      "refresh 1923\n",
      "slew 1924\n",
      "behavior 1925\n",
      "jock 1926\n",
      "testosteron 1927\n",
      "driven 1928\n",
      "musclehead 1929\n",
      "opinion 1930\n",
      "gritti 1931\n",
      "realism 1932\n",
      "curs 1933\n",
      "aggress 1934\n",
      "encourag 1935\n",
      "pursu 1936\n",
      "lake 1937\n",
      "tx 1938\n",
      "b4 1939\n",
      "overboard 1940\n",
      "denni 1941\n",
      "quaid 1942\n",
      "underr 1943\n",
      "kudo 1944\n",
      "practic 1945\n",
      "dugout 1946\n",
      "rush 1947\n",
      "pitcher 1948\n",
      "bench 1949\n",
      "pace 1950\n",
      "instant 1951\n",
      "liner 1952\n",
      "produc 1953\n",
      "dove 1954\n",
      "pickup 1955\n",
      "truck 1956\n",
      "full 1957\n",
      "antifreez 1958\n",
      "gape 1959\n",
      "bloodi 1960\n",
      "wound 1961\n",
      "oop 1962\n",
      "forget 1963\n",
      "shrug 1964\n",
      "impal 1965\n",
      "drench 1966\n",
      "expos 1967\n",
      "forgot 1968\n",
      "victori 1969\n",
      "r 1970\n",
      "blackmor 1971\n",
      "novel 1972\n",
      "substitut 1973\n",
      "2001 1974\n",
      "prove 1975\n",
      "authent 1976\n",
      "sean 1977\n",
      "bean 1978\n",
      "contriv 1979\n",
      "triangl 1980\n",
      "ridd 1981\n",
      "lorna 1982\n",
      "doon 1983\n",
      "ladi 1984\n",
      "dugal 1985\n",
      "whichev 1986\n",
      "prefer 1987\n",
      "carver 1988\n",
      "expand 1989\n",
      "upon 1990\n",
      "key 1991\n",
      "counsellor 1992\n",
      "youngest 1993\n",
      "lizzi 1994\n",
      "crucial 1995\n",
      "screenplay 1996\n",
      "convict 1997\n",
      "clive 1998\n",
      "owen 1999\n",
      "handsom 2000\n",
      "gosford 2001\n",
      "park 2002\n",
      "arthur 2003\n",
      "confirm 2004\n",
      "stoic 2005\n",
      "wig 2006\n",
      "suit 2007\n",
      "polli 2008\n",
      "walker 2009\n",
      "accomplish 2010\n",
      "april 2011\n",
      "costar 2012\n",
      "colorless 2013\n",
      "lacklust 2014\n",
      "cold 2015\n",
      "sore 2016\n",
      "hide 2017\n",
      "costum 2018\n",
      "ensor 2019\n",
      "granddaught 2020\n",
      "reput 2021\n",
      "sceneri 2022\n",
      "brown 2023\n",
      "gray 2024\n",
      "barren 2025\n",
      "hint 2026\n",
      "sunni 2027\n",
      "sky 2028\n",
      "southwest 2029\n",
      "england 2030\n",
      "green 2031\n",
      "sunshin 2032\n",
      "faggu 2033\n",
      "aidan 2034\n",
      "gillen 2035\n",
      "villain 2036\n",
      "adapt 2037\n",
      "condens 2038\n",
      "research 2039\n",
      "investig 2040\n",
      "ancient 2041\n",
      "scroll 2042\n",
      "press 2043\n",
      "laid 2044\n",
      "arama 2045\n",
      "peru 2046\n",
      "text 2047\n",
      "anonym 2048\n",
      "destroy 2049\n",
      "truth 2050\n",
      "hater 2051\n",
      "church 2052\n",
      "local 2053\n",
      "govern 2054\n",
      "hundr 2055\n",
      "dozen 2056\n",
      "roman 2057\n",
      "cathol 2058\n",
      "bent 2059\n",
      "movement 2060\n",
      "crock 2061\n",
      "doodi 2062\n",
      "faith 2063\n",
      "typic 2064\n",
      "pattern 2065\n",
      "scam 2066\n",
      "huckster 2067\n",
      "document 2068\n",
      "joseph 2069\n",
      "smith 2070\n",
      "heaven 2071\n",
      "therefor 2072\n",
      "museum 2073\n",
      "smithsonian 2074\n",
      "specialist 2075\n",
      "coptic 2076\n",
      "smart 2077\n",
      "desir 2078\n",
      "mislead 2079\n",
      "public 2080\n",
      "agey 2081\n",
      "hokum 2082\n",
      "juda 2083\n",
      "gnostic 2084\n",
      "carbon 2085\n",
      "dilig 2086\n",
      "deem 2087\n",
      "expert 2088\n",
      "referenc 2089\n",
      "heret 2090\n",
      "300 2091\n",
      "celestin 2092\n",
      "propheci 2093\n",
      "evolv 2094\n",
      "optim 2095\n",
      "philosophi 2096\n",
      "built 2097\n",
      "foundat 2098\n",
      "sand 2099\n",
      "harsh 2100\n",
      "ugli 2101\n",
      "overpopul 2102\n",
      "cruel 2103\n",
      "blade 2104\n",
      "runner 2105\n",
      "horizon 2106\n",
      "utopian 2107\n",
      "hippi 2108\n",
      "commun 2109\n",
      "peac 2110\n",
      "piffl 2111\n",
      "wish 2112\n",
      "god 2113\n",
      "vision 2114\n",
      "spiritu 2115\n",
      "evolut 2116\n",
      "exist 2117\n",
      "darwininan 2118\n",
      "becam 2119\n",
      "older 2120\n",
      "1800 2121\n",
      "nazareth 2122\n",
      "ager 2123\n",
      "unusu 2124\n",
      "wiccan 2125\n",
      "brand 2126\n",
      "magic 2127\n",
      "witchcraft 2128\n",
      "stone 2129\n",
      "shadow 2130\n",
      "prior 2131\n",
      "gerald 2132\n",
      "gardner 2133\n",
      "1900 2134\n",
      "buddi 2135\n",
      "aleist 2136\n",
      "crowley 2137\n",
      "guru 2138\n",
      "teach 2139\n",
      "thumb 2140\n",
      "nose 2141\n",
      "perpetr 2142\n",
      "fraud 2143\n",
      "simpl 2144\n",
      "fierc 2145\n",
      "hatr 2146\n",
      "competit 2147\n",
      "oppos 2148\n",
      "struggl 2149\n",
      "wallet 2150\n",
      "instinct 2151\n",
      "guidanc 2152\n",
      "wholeheartedli 2153\n",
      "warm 2154\n",
      "uplift 2155\n",
      "outstand 2156\n",
      "alisan 2157\n",
      "porter 2158\n",
      "curli 2159\n",
      "sue 2160\n",
      "immens 2161\n",
      "mplex 2162\n",
      "assum 2163\n",
      "prais 2164\n",
      "greatest 2165\n",
      "opera 2166\n",
      "somewher 2167\n",
      "wagner 2168\n",
      "cultur 2169\n",
      "represent 2170\n",
      "swan 2171\n",
      "unmitig 2172\n",
      "leaden 2173\n",
      "tricksi 2174\n",
      "lugubri 2175\n",
      "matter 2176\n",
      "shakespear 2177\n",
      "allow 2178\n",
      "anywher 2179\n",
      "studio 2180\n",
      "syberberg 2181\n",
      "smallest 2182\n",
      "justif 2183\n",
      "parsif 2184\n",
      "bisexu 2185\n",
      "integr 2186\n",
      "stage 2187\n",
      "transmut 2188\n",
      "beatnik 2189\n",
      "babe 2190\n",
      "tenor 2191\n",
      "singer 2192\n",
      "doubl 2193\n",
      "dose 2194\n",
      "armin 2195\n",
      "jordan 2196\n",
      "conductor 2197\n",
      "amforta 2198\n",
      "monstrous 2199\n",
      "exposur 2200\n",
      "batonzilla 2201\n",
      "ate 2202\n",
      "monsalvat 2203\n",
      "friday 2204\n",
      "transcend 2205\n",
      "loveli 2206\n",
      "repres 2207\n",
      "scatter 2208\n",
      "shopworn 2209\n",
      "flaccid 2210\n",
      "crocus 2211\n",
      "ill 2212\n",
      "turf 2213\n",
      "expedi 2214\n",
      "baffl 2215\n",
      "imperfect 2216\n",
      "splice 2217\n",
      "gurnemanz 2218\n",
      "mountain 2219\n",
      "pastur 2220\n",
      "lush 2221\n",
      "andrew 2222\n",
      "endur 2223\n",
      "trumpet 2224\n",
      "aural 2225\n",
      "glare 2226\n",
      "fatigu 2227\n",
      "impati 2228\n",
      "uninspir 2229\n",
      "conduct 2230\n",
      "paralyt 2231\n",
      "ritual 2232\n",
      "1951 2233\n",
      "bayreuth 2234\n",
      "record 2235\n",
      "knappertsbusch 2236\n",
      "tempi 2237\n",
      "altogeth 2238\n",
      "puls 2239\n",
      "ebb 2240\n",
      "orchestr 2241\n",
      "superior 2242\n",
      "major 2243\n",
      "tie 2244\n",
      "loos 2245\n",
      "lock 2246\n",
      "snatch 2247\n",
      "viewpoint 2248\n",
      "statham 2249\n",
      "lift 2250\n",
      "specif 2251\n",
      "ray 2252\n",
      "liotta 2253\n",
      "speedo 2254\n",
      "afterward 2255\n",
      "symbol 2256\n",
      "ritchi 2257\n",
      "plastic 2258\n",
      "surgeon 2259\n",
      "expertis 2260\n",
      "dentist 2261\n",
      "colgat 2262\n",
      "whiten 2263\n",
      "crazi 2264\n",
      "ga 2265\n",
      "station 2266\n",
      "stare 2267\n",
      "ahead 2268\n",
      "pretend 2269\n",
      "shout 2270\n",
      "robot 2271\n",
      "tone 2272\n",
      "nymph 2273\n",
      "cure 2274\n",
      "gingiv 2275\n",
      "appar 2276\n",
      "accord 2277\n",
      "caus 2278\n",
      "nasti 2279\n",
      "red 2280\n",
      "rash 2281\n",
      "chunk 2282\n",
      "tcp 2283\n",
      "sophi 2284\n",
      "holland 2285\n",
      "baaaaaad 2286\n",
      "wanna 2287\n",
      "meanest 2288\n",
      "cow 2289\n",
      "planet 2290\n",
      "sarcast 2291\n",
      "petti 2292\n",
      "sulk 2293\n",
      "6 2294\n",
      "yr 2295\n",
      "attitud 2296\n",
      "angl 2297\n",
      "blair 2298\n",
      "judd 2299\n",
      "hack 2300\n",
      "door 2301\n",
      "unlock 2302\n",
      "laughabl 2303\n",
      "spoof 2304\n",
      "describ 2305\n",
      "ts 2306\n",
      "run 2307\n",
      "drip 2308\n",
      "raspberri 2309\n",
      "juic 2310\n",
      "ear 2311\n",
      "mouth 2312\n",
      "savini 2313\n",
      "tabl 2314\n",
      "directli 2315\n",
      "funnier 2316\n",
      "damn 2317\n",
      "tractor 2318\n",
      "thee 2319\n",
      "slice 2320\n",
      "wrist 2321\n",
      "weaken 2322\n",
      "silliest 2323\n",
      "molli 2324\n",
      "wander 2325\n",
      "lo 2326\n",
      "behold 2327\n",
      "shaun 2328\n",
      "hutson 2329\n",
      "promot 2330\n",
      "altho 2331\n",
      "admittedli 2332\n",
      "fav 2333\n",
      "xplanat 2334\n",
      "lure 2335\n",
      "round 2336\n",
      "petrol 2337\n",
      "dazzl 2338\n",
      "ventur 2339\n",
      "cue 2340\n",
      "stupidli 2341\n",
      "slappin 2342\n",
      "pour 2343\n",
      "everywher 2344\n",
      "endeth 2345\n",
      "conclus 2346\n",
      "aim 2347\n",
      "99 2348\n",
      "english 2349\n",
      "15 2350\n",
      "min 2351\n",
      "field 2352\n",
      "area 2353\n",
      "christ 2354\n",
      "testicl 2355\n",
      "dusk 2356\n",
      "til 2357\n",
      "dawn 2358\n",
      "option 2359\n",
      "0 2360\n",
      "woulda 2361\n",
      "cuz 2362\n",
      "cheat 2363\n",
      "88 2364\n",
      "clean 2365\n",
      "couch 2366\n",
      "leviticu 2367\n",
      "irish 2368\n",
      "deliv 2369\n",
      "wash 2370\n",
      "haggard 2371\n",
      "sprinkl 2372\n",
      "grant 2373\n",
      "gore 2374\n",
      "troubl 2375\n",
      "organ 2376\n",
      "yank 2377\n",
      "formerli 2378\n",
      "tight 2379\n",
      "bodi 2380\n",
      "horrif 2381\n",
      "inhuman 2382\n",
      "wit 2383\n",
      "linear 2384\n",
      "contempl 2385\n",
      "grasp 2386\n",
      "explain 2387\n",
      "soundtrack 2388\n",
      "ordinari 2389\n",
      "horni 2390\n",
      "teen 2391\n",
      "intestin 2392\n",
      "mutant 2393\n",
      "disembowel 2394\n",
      "solv 2395\n",
      "accent 2396\n",
      "uggh 2397\n",
      "technic 2398\n",
      "7 2399\n",
      "valu 2400\n",
      "50 2401\n",
      "unlik 2402\n",
      "st 2403\n",
      "longest 2404\n",
      "quickli 2405\n",
      "result 2406\n",
      "overblown 2407\n",
      "steadi 2408\n",
      "diet 2409\n",
      "meringu 2410\n",
      "streetcar 2411\n",
      "sun 2412\n",
      "african 2413\n",
      "ace 2414\n",
      "hole 2415\n",
      "categori 2416\n",
      "amaz 2417\n",
      "pari 2418\n",
      "speak 2419\n",
      "mostli 2420\n",
      "gilligan 2421\n",
      "island 2422\n",
      "promis 2423\n",
      "shown 2424\n",
      "dana 2425\n",
      "plato 2426\n",
      "disneylik 2427\n",
      "disregard 2428\n",
      "meaningless 2429\n",
      "meant 2430\n",
      "wont 2431\n",
      "fishermen 2432\n",
      "central 2433\n",
      "initi 2434\n",
      "jam 2435\n",
      "overli 2436\n",
      "clock 2437\n",
      "95 2438\n",
      "six 2439\n",
      "eighti 2440\n",
      "silver 2441\n",
      "center 2442\n",
      "link 2443\n",
      "nikhil 2444\n",
      "advani 2445\n",
      "surprisingli 2446\n",
      "1950 2447\n",
      "clash 2448\n",
      "wayn 2449\n",
      "attach 2450\n",
      "diplomat 2451\n",
      "intent 2452\n",
      "bull 2453\n",
      "harrod 2454\n",
      "passiv 2455\n",
      "bluff 2456\n",
      "facial 2457\n",
      "subtl 2458\n",
      "consider 2459\n",
      "domin 2460\n",
      "cropper 2461\n",
      "nobodi 2462\n",
      "intermitt 2463\n",
      "narr 2464\n",
      "titular 2465\n",
      "geisha 2466\n",
      "curri 2467\n",
      "favour 2468\n",
      "isolationist 2469\n",
      "swashbuckl 2470\n",
      "leather 2471\n",
      "accuraci 2472\n",
      "sensibl 2473\n",
      "demonstr 2474\n",
      "choreograph 2475\n",
      "ceremoni 2476\n",
      "entail 2477\n",
      "observ 2478\n",
      "prop 2479\n",
      "appli 2480\n",
      "fractur 2481\n",
      "ad 2482\n",
      "flag 2483\n",
      "wave 2484\n",
      "wordi 2485\n",
      "japanes 2486\n",
      "lengthi 2487\n",
      "interfer 2488\n",
      "plenti 2489\n",
      "dress 2490\n",
      "caucasian 2491\n",
      "inn 2492\n",
      "sixth 2493\n",
      "frankli 2494\n",
      "laden 2495\n",
      "anti 2496\n",
      "communist 2497\n",
      "subtext 2498\n",
      "confess 2499\n",
      "outing 2500\n",
      "flight 2501\n",
      "plane 2502\n",
      "flick 2503\n",
      "afterlif 2504\n",
      "uncomfort 2505\n",
      "loop 2506\n",
      "etern 2507\n",
      "next 2508\n",
      "mano 2509\n",
      "crummi 2510\n",
      "moron 2511\n",
      "duo 2512\n",
      "threw 2513\n",
      "relief 2514\n",
      "focus 2515\n",
      "fair 2516\n",
      "crown 2517\n",
      "intern 2518\n",
      "edit 2519\n",
      "honestli 2520\n",
      "hurt 2521\n",
      "100 2522\n",
      "enemi 2523\n",
      "campi 2524\n",
      "nail 2525\n",
      "freebird 2526\n",
      "reliv 2527\n",
      "misspent 2528\n",
      "youth 2529\n",
      "biker 2530\n",
      "pigeon 2531\n",
      "kinda 2532\n",
      "trailer 2533\n",
      "u 2534\n",
      "k 2535\n",
      "tpb 2536\n",
      "rk 2537\n",
      "indian 2538\n",
      "ab 2539\n",
      "akshay 2540\n",
      "bhoomika 2541\n",
      "hype 2542\n",
      "filmfar 2543\n",
      "khake 2544\n",
      "khakke 2545\n",
      "fairli 2546\n",
      "cant 2547\n",
      "waqt 2548\n",
      "comput 2549\n",
      "stood 2550\n",
      "test 2551\n",
      "twelv 2552\n",
      "graphic 2553\n",
      "explor 2554\n",
      "cosbi 2555\n",
      "cb 2556\n",
      "hit 2557\n",
      "madelin 2558\n",
      "kahn 2559\n",
      "pass 2560\n",
      "tb 2561\n",
      "strongli 2562\n",
      "air 2563\n",
      "yesterday 2564\n",
      "fri 2565\n",
      "notic 2566\n",
      "swedish 2567\n",
      "sceptic 2568\n",
      "paradigm 2569\n",
      "shift 2570\n",
      "mankind 2571\n",
      "villag 2572\n",
      "misunderstand 2573\n",
      "courag 2574\n",
      "disguis 2575\n",
      "brutal 2576\n",
      "abil 2577\n",
      "trigger 2578\n",
      "outsid 2579\n",
      "mask 2580\n",
      "be 2581\n",
      "gabriella 2582\n",
      "highlight 2583\n",
      "kay 2584\n",
      "pollak 2585\n",
      "factor 2586\n",
      "amor 2587\n",
      "engag 2588\n",
      "marit 2589\n",
      "hijink 2590\n",
      "flashi 2591\n",
      "compromis 2592\n",
      "aimlessli 2593\n",
      "countrysid 2594\n",
      "hapless 2595\n",
      "rescu 2596\n",
      "pointless 2597\n",
      "uk 2598\n",
      "sadli 2599\n",
      "deceas 2600\n",
      "ian 2601\n",
      "charleson 2602\n",
      "chariot 2603\n",
      "celluloid 2604\n",
      "abomin 2605\n",
      "certainli 2606\n",
      "superbl 2607\n",
      "amusingli 2608\n",
      "albert 2609\n",
      "maysl 2610\n",
      "grey 2611\n",
      "garden 2612\n",
      "gentrif 2613\n",
      "east 2614\n",
      "hampton 2615\n",
      "brother 2616\n",
      "edith 2617\n",
      "bouvier 2618\n",
      "beal 2619\n",
      "aunt 2620\n",
      "cousin 2621\n",
      "jacquelin 2622\n",
      "kennedi 2623\n",
      "onassi 2624\n",
      "pirouett 2625\n",
      "majorett 2626\n",
      "cat 2627\n",
      "ted 2628\n",
      "z 2629\n",
      "shortli 2630\n",
      "februari 2631\n",
      "1977 2632\n",
      "82 2633\n",
      "experienc 2634\n",
      "sang 2635\n",
      "nightclub 2636\n",
      "jr 2637\n",
      "1925 2638\n",
      "miami 2639\n",
      "beach 2640\n",
      "spellbound 2641\n",
      "cordial 2642\n",
      "jack 2643\n",
      "lemmon 2644\n",
      "felix 2645\n",
      "hypochondriac 2646\n",
      "cook 2647\n",
      "longer 2648\n",
      "walter 2649\n",
      "matthau 2650\n",
      "untidi 2651\n",
      "unreli 2652\n",
      "sport 2653\n",
      "report 2654\n",
      "divorc 2655\n",
      "ex 2656\n",
      "bachelor 2657\n",
      "distress 2658\n",
      "contrari 2659\n",
      "disorderli 2660\n",
      "exhibit 2661\n",
      "mania 2662\n",
      "clown 2663\n",
      "tragi 2664\n",
      "invit 2665\n",
      "neighbour 2666\n",
      "supper 2667\n",
      "softer 2668\n",
      "bowl 2669\n",
      "prepar 2670\n",
      "drink 2671\n",
      "weather 2672\n",
      "subject 2673\n",
      "weep 2674\n",
      "improv 2675\n",
      "invest 2676\n",
      "ranger 2677\n",
      "fifteen 2678\n",
      "knight 2679\n",
      "eras 2680\n",
      "resum 2681\n",
      "roommat 2682\n",
      "pray 2683\n",
      "sam 2684\n",
      "fuller 2685\n",
      "street 2686\n",
      "explos 2687\n",
      "endlessli 2688\n",
      "widmark 2689\n",
      "pickpocket 2690\n",
      "skip 2691\n",
      "mccoy 2692\n",
      "finger 2693\n",
      "slide 2694\n",
      "grift 2695\n",
      "gangster 2696\n",
      "moll 2697\n",
      "candi 2698\n",
      "peter 2699\n",
      "microfilm 2700\n",
      "invalu 2701\n",
      "boyfriend 2702\n",
      "kiley 2703\n",
      "deliveri 2704\n",
      "tasti 2705\n",
      "psycho 2706\n",
      "unpredict 2707\n",
      "curious 2708\n",
      "charm 2709\n",
      "bogart 2710\n",
      "mitchum 2711\n",
      "lone 2712\n",
      "maintain 2713\n",
      "lacon 2714\n",
      "drawn 2715\n",
      "trap 2716\n",
      "torn 2717\n",
      "polic 2718\n",
      "commi 2719\n",
      "law 2720\n",
      "fatal 2721\n",
      "fourth 2722\n",
      "rap 2723\n",
      "sheet 2724\n",
      "gorgeou 2725\n",
      "tramp 2726\n",
      "altern 2727\n",
      "slap 2728\n",
      "edg 2729\n",
      "lacquer 2730\n",
      "glamour 2731\n",
      "lana 2732\n",
      "turner 2733\n",
      "asset 2734\n",
      "vulner 2735\n",
      "knock 2736\n",
      "wake 2737\n",
      "beer 2738\n",
      "tender 2739\n",
      "thelma 2740\n",
      "ritter 2741\n",
      "stooli 2742\n",
      "moe 2743\n",
      "heartbreakingli 2744\n",
      "honest 2745\n",
      "stun 2746\n",
      "locat 2747\n",
      "particularli 2748\n",
      "cloister 2749\n",
      "enhanc 2750\n",
      "tension 2751\n",
      "afraid 2752\n",
      "linger 2753\n",
      "stunningli 2754\n",
      "maximum 2755\n",
      "bold 2756\n",
      "uncompromisingli 2757\n",
      "columbin 2758\n",
      "massacr 2759\n",
      "worri 2760\n",
      "guid 2761\n",
      "serial 2762\n",
      "killer 2763\n",
      "threat 2764\n",
      "blew 2765\n",
      "estim 2766\n",
      "wide 2767\n",
      "rendit 2768\n",
      "stanley 2769\n",
      "kubrick 2770\n",
      "nicholson 2771\n",
      "chilli 2772\n",
      "torranc 2773\n",
      "overlook 2774\n",
      "hotel 2775\n",
      "canada 2776\n",
      "unaffect 2777\n",
      "gruesom 2778\n",
      "wendi 2779\n",
      "shelley 2780\n",
      "duval 2781\n",
      "lloyd 2782\n",
      "told 2783\n",
      "dick 2784\n",
      "hallorann 2785\n",
      "scatman 2786\n",
      "crother 2787\n",
      "endow 2788\n",
      "psychic 2789\n",
      "energi 2790\n",
      "flash 2791\n",
      "among 2792\n",
      "inhabit 2793\n",
      "immedi 2794\n",
      "increasingli 2795\n",
      "affect 2796\n",
      "comfort 2797\n",
      "defin 2798\n",
      "insan 2799\n",
      "dread 2800\n",
      "focu 2801\n",
      "creativ 2802\n",
      "bloodbath 2803\n",
      "critic 2804\n",
      "omit 2805\n",
      "subplot 2806\n",
      "led 2807\n",
      "climax 2808\n",
      "omiss 2809\n",
      "necessari 2810\n",
      "redempt 2811\n",
      "albeit 2812\n",
      "unconvent 2813\n",
      "analyst 2814\n",
      "decay 2815\n",
      "soft 2816\n",
      "spot 2817\n",
      "mental 2818\n",
      "subsequ 2819\n",
      "unbal 2820\n",
      "threaten 2821\n",
      "approach 2822\n",
      "steadicam 2823\n",
      "ghostli 2824\n",
      "twin 2825\n",
      "rais 2826\n",
      "trademark 2827\n",
      "grin 2828\n",
      "undermin 2829\n",
      "goofi 2830\n",
      "martin 2831\n",
      "sheen 2832\n",
      "ideal 2833\n",
      "terrifi 2834\n",
      "johnni 2835\n",
      "imposs 2836\n",
      "1997 2837\n",
      "indel 2838\n",
      "flew 2839\n",
      "cuckoo 2840\n",
      "nest 2841\n",
      "alongsid 2842\n",
      "harvey 2843\n",
      "omen 2844\n",
      "damien 2845\n",
      "haley 2846\n",
      "joel 2847\n",
      "osment 2848\n",
      "child 2849\n",
      "icon 2850\n",
      "counterpart 2851\n",
      "scream 2852\n",
      "ought 2853\n",
      "1980 2854\n",
      "secondari 2855\n",
      "fear 2856\n",
      "emerg 2857\n",
      "irvin 2858\n",
      "welsh 2859\n",
      "trainspot 2860\n",
      "literari 2861\n",
      "liberti 2862\n",
      "boyl 2863\n",
      "junki 2864\n",
      "brought 2865\n",
      "necessarili 2866\n",
      "approxim 2867\n",
      "nichlolson 2868\n",
      "masterclass 2869\n",
      "current 2870\n",
      "finer 2871\n",
      "woodi 2872\n",
      "bogdonovich 2873\n",
      "manhattan 2874\n",
      "peer 2875\n",
      "hollow 2876\n",
      "bangster 2877\n",
      "duster 2878\n",
      "cloth 2879\n",
      "kerchief 2880\n",
      "somehow 2881\n",
      "protect 2882\n",
      "dust 2883\n",
      "heat 2884\n",
      "horsi 2885\n",
      "suppli 2886\n",
      "bedroom 2887\n",
      "anymor 2888\n",
      "ice 2889\n",
      "brief 2890\n",
      "rundown 2891\n",
      "shrink 2892\n",
      "busi 2893\n",
      "trite 2894\n",
      "shoulder 2895\n",
      "obnoxi 2896\n",
      "maci 2897\n",
      "grappl 2898\n",
      "marriag 2899\n",
      "repress 2900\n",
      "bedtim 2901\n",
      "beyond 2902\n",
      "interact 2903\n",
      "accid 2904\n",
      "newcom 2905\n",
      "rural 2906\n",
      "pet 2907\n",
      "cemeteri 2908\n",
      "dale 2909\n",
      "midkiff 2910\n",
      "micmac 2911\n",
      "burial 2912\n",
      "ground 2913\n",
      "resurrect 2914\n",
      "buri 2915\n",
      "averag 2916\n",
      "clumsi 2917\n",
      "inept 2918\n",
      "worthless 2919\n",
      "dismiss 2920\n",
      "fred 2921\n",
      "gwynn 2922\n",
      "wise 2923\n",
      "brad 2924\n",
      "greenquist 2925\n",
      "disfigur 2926\n",
      "victor 2927\n",
      "pascow 2928\n",
      "cameo 2929\n",
      "minist 2930\n",
      "lambert 2931\n",
      "mainstream 2932\n",
      "strictli 2933\n",
      "ballroom 2934\n",
      "shall 2935\n",
      "pleasingli 2936\n",
      "upset 2937\n",
      "grammar 2938\n",
      "inher 2939\n",
      "incorrect 2940\n",
      "caribbean 2941\n",
      "canadian 2942\n",
      "subtitl 2943\n",
      "speaker 2944\n",
      "automat 2945\n",
      "flavour 2946\n",
      "toronto 2947\n",
      "heroin 2948\n",
      "atyp 2949\n",
      "routin 2950\n",
      "mum 2951\n",
      "address 2952\n",
      "subtli 2953\n",
      "characterist 2954\n",
      "summari 2955\n",
      "grew 2956\n",
      "narrow 2957\n",
      "difficulti 2958\n",
      "13th 2959\n",
      "australian 2960\n",
      "burnt 2961\n",
      "accident 2962\n",
      "eildon 2963\n",
      "grown 2964\n",
      "reveng 2965\n",
      "ridden 2966\n",
      "curios 2967\n",
      "alan 2968\n",
      "soap 2969\n",
      "x 2970\n",
      "file 2971\n",
      "british 2972\n",
      "imperialist 2973\n",
      "feather 2974\n",
      "charg 2975\n",
      "brigad 2976\n",
      "cream 2977\n",
      "crop 2978\n",
      "reflect 2979\n",
      "empir 2980\n",
      "visit 2981\n",
      "alabama 2982\n",
      "photograph 2983\n",
      "march 2984\n",
      "unchang 2985\n",
      "encroach 2986\n",
      "vandal 2987\n",
      "din 2988\n",
      "stretcher 2989\n",
      "lazarushian 2990\n",
      "gunga 2991\n",
      "belt 2992\n",
      "flay 2993\n",
      "livin 2994\n",
      "gawd 2995\n",
      "54 2996\n",
      "misti 2997\n",
      "liar 2998\n",
      "rang 2999\n",
      "mice 3000\n",
      "likabl 3001\n",
      "leo 3002\n",
      "will 3003\n",
      "unprotect 3004\n",
      "awar 3005\n",
      "hiv 3006\n",
      "statu 3007\n",
      "reject 3008\n",
      "medicin 3009\n",
      "avail 3010\n",
      "marcel 3011\n",
      "train 3012\n",
      "gay 3013\n",
      "bash 3014\n",
      "unawar 3015\n",
      "consequ 3016\n",
      "friendli 3017\n",
      "unwil 3018\n",
      "funer 3019\n",
      "melodrama 3020\n",
      "1930 3021\n",
      "backstreet 3022\n",
      "withstand 3023\n",
      "sidney 3024\n",
      "lumet 3025\n",
      "utmost 3026\n",
      "kelli 3027\n",
      "masterson 3028\n",
      "mode 3029\n",
      "climact 3030\n",
      "suspens 3031\n",
      "disbelief 3032\n",
      "paramount 3033\n",
      "heist 3034\n",
      "bleak 3035\n",
      "unforgiv 3036\n",
      "rivet 3037\n",
      "chalk 3038\n",
      "botch 3039\n",
      "mom 3040\n",
      "pop 3041\n",
      "jewelri 3042\n",
      "formid 3043\n",
      "assembl 3044\n",
      "specialti 3045\n",
      "philip 3046\n",
      "seymour 3047\n",
      "hoffman 3048\n",
      "ethan 3049\n",
      "hawk 3050\n",
      "financi 3051\n",
      "f 3052\n",
      "former 3053\n",
      "fool 3054\n",
      "proof 3055\n",
      "westchest 3056\n",
      "shadi 3057\n",
      "murder 3058\n",
      "greek 3059\n",
      "scale 3060\n",
      "finney 3061\n",
      "materi 3062\n",
      "dog 3063\n",
      "afternoon 3064\n",
      "inabl 3065\n",
      "fulfil 3066\n",
      "splendidli 3067\n",
      "aforement 3068\n",
      "ten 3069\n",
      "calm 3070\n",
      "demeanor 3071\n",
      "reveal 3072\n",
      "oper 3073\n",
      "smaller 3074\n",
      "tomei 3075\n",
      "limit 3076\n",
      "meanwhil 3077\n",
      "loser 3078\n",
      "debt 3079\n",
      "babi 3080\n",
      "straw 3081\n",
      "fish 3082\n",
      "terrifyingli 3083\n",
      "tragic 3084\n",
      "recoil 3085\n",
      "ditsi 3086\n",
      "pervers 3087\n",
      "marisa 3088\n",
      "bug 3089\n",
      "shannon 3090\n",
      "ass 3091\n",
      "ami 3092\n",
      "brian 3093\n",
      "byrn 3094\n",
      "rosemari 3095\n",
      "harri 3096\n",
      "exact 3097\n",
      "forgiv 3098\n",
      "showi 3099\n",
      "perpetu 3100\n",
      "downward 3101\n",
      "spiral 3102\n",
      "serpico 3103\n",
      "pawnbrok 3104\n",
      "12 3105\n",
      "angri 3106\n",
      "queri 3107\n",
      "commit 3108\n",
      "doom 3109\n",
      "bomber 3110\n",
      "pilot 3111\n",
      "heavenli 3112\n",
      "fog 3113\n",
      "brush 3114\n",
      "celesti 3115\n",
      "panach 3116\n",
      "niven 3117\n",
      "hunter 3118\n",
      "plight 3119\n",
      "radio 3120\n",
      "heighten 3121\n",
      "cardiff 3122\n",
      "photographi 3123\n",
      "suprem 3124\n",
      "extend 3125\n",
      "earthbound 3126\n",
      "technicolour 3127\n",
      "mariu 3128\n",
      "highpoint 3129\n",
      "travel 3130\n",
      "slant 3131\n",
      "billingham 3132\n",
      "ordinarili 3133\n",
      "choos 3134\n",
      "phone 3135\n",
      "booth 3136\n",
      "ingredi 3137\n",
      "suffic 3138\n",
      "blue 3139\n",
      "ambient 3140\n",
      "rouveroy 3141\n",
      "booz 3142\n",
      "immers 3143\n",
      "filth 3144\n",
      "defend 3145\n",
      "achiev 3146\n",
      "listen 3147\n",
      "commentari 3148\n",
      "track 3149\n",
      "disbeliev 3150\n",
      "fals 3151\n",
      "beg 3152\n",
      "kenneth 3153\n",
      "branagh 3154\n",
      "concess 3155\n",
      "stud 3156\n",
      "wider 3157\n",
      "gloriou 3158\n",
      "discret 3159\n",
      "bard 3160\n",
      "overjoy 3161\n",
      "display 3162\n",
      "grief 3163\n",
      "anger 3164\n",
      "derek 3165\n",
      "jacobi 3166\n",
      "wili 3167\n",
      "claudiu 3168\n",
      "decept 3169\n",
      "treacheri 3170\n",
      "christi 3171\n",
      "gertrud 3172\n",
      "judi 3173\n",
      "billi 3174\n",
      "crystal 3175\n",
      "robin 3176\n",
      "undeni 3177\n",
      "whale 3178\n",
      "monument 3179\n",
      "scholar 3180\n",
      "digest 3181\n",
      "kasdan 3182\n",
      "convent 3183\n",
      "structur 3184\n",
      "absorb 3185\n",
      "canva 3186\n",
      "illumin 3187\n",
      "onto 3188\n",
      "jettison 3189\n",
      "favor 3190\n",
      "ongoing 3191\n",
      "insist 3192\n",
      "miracul 3193\n",
      "miracl 3194\n",
      "random 3195\n",
      "incid 3196\n",
      "connect 3197\n",
      "interconnect 3198\n",
      "bucket 3199\n",
      "underscor 3200\n",
      "lightli 3201\n",
      "bolster 3202\n",
      "mac 3203\n",
      "shape 3204\n",
      "easi 3205\n",
      "front 3206\n",
      "wrought 3207\n",
      "medit 3208\n",
      "constraint 3209\n",
      "dreamcatch 3210\n",
      "larri 3211\n",
      "ration 3212\n",
      "succeed 3213\n",
      "romanian 3214\n",
      "vulgar 3215\n",
      "uneduc 3216\n",
      "relish 3217\n",
      "superfici 3218\n",
      "garcea 3219\n",
      "vacanta 3220\n",
      "mare 3221\n",
      "mugur 3222\n",
      "mih 3223\n",
      "escu 3224\n",
      "doru 3225\n",
      "octavian 3226\n",
      "dumitru 3227\n",
      "sub 3228\n",
      "artisan 3229\n",
      "presum 3230\n",
      "pintili 3231\n",
      "40 3232\n",
      "dumin 3233\n",
      "ora 3234\n",
      "sase 3235\n",
      "reconstituirea 3236\n",
      "declar 3237\n",
      "lenght 3238\n",
      "cinematograph 3239\n",
      "niki 3240\n",
      "ardelean 3241\n",
      "sampl 3242\n",
      "merit 3243\n",
      "indi 3244\n",
      "nichol 3245\n",
      "closer 3246\n",
      "adrian 3247\n",
      "grenier 3248\n",
      "rosario 3249\n",
      "dawson 3250\n",
      "intens 3251\n",
      "anna 3252\n",
      "coincid 3253\n",
      "patrick 3254\n",
      "marber 3255\n",
      "supposedli 3256\n",
      "schnitzler 3257\n",
      "reigen 3258\n",
      "poster 3259\n",
      "fli 3260\n",
      "bryant 3261\n",
      "haliday 3262\n",
      "imdb 3263\n",
      "vincent 3264\n",
      "price 3265\n",
      "micheal 3266\n",
      "gough 3267\n",
      "mood 3268\n",
      "hammer 3269\n",
      "teleport 3270\n",
      "rummag 3271\n",
      "remaind 3272\n",
      "bin 3273\n",
      "rib 3274\n",
      "mst3k 3275\n",
      "crew 3276\n",
      "tpm 3277\n",
      "bog 3278\n",
      "momentum 3279\n",
      "mire 3280\n",
      "nonsens 3281\n",
      "fund 3282\n",
      "univers 3283\n",
      "lembach 3284\n",
      "sinist 3285\n",
      "cabal 3286\n",
      "machin 3287\n",
      "unconvinc 3288\n",
      "transpar 3289\n",
      "espresso 3290\n",
      "rat 3291\n",
      "mutil 3292\n",
      "wear 3293\n",
      "diaper 3294\n",
      "rubber 3295\n",
      "cement 3296\n",
      "electrocut 3297\n",
      "london 3298\n",
      "path 3299\n",
      "cruis 3300\n",
      "eric 3301\n",
      "bullhorn 3302\n",
      "inject 3303\n",
      "turgid 3304\n",
      "corrupt 3305\n",
      "administr 3306\n",
      "fret 3307\n",
      "fume 3308\n",
      "hiss 3309\n",
      "blackmail 3310\n",
      "werewolf 3311\n",
      "outfit 3312\n",
      "tweed 3313\n",
      "tattersal 3314\n",
      "vest 3315\n",
      "assist 3316\n",
      "girlfriend 3317\n",
      "secretari 3318\n",
      "parad 3319\n",
      "gel 3320\n",
      "eh 3321\n",
      "compil 3322\n",
      "harmless 3323\n",
      "divers 3324\n",
      "consum 3325\n",
      "snack 3326\n",
      "saturday 3327\n",
      "seduct 3328\n",
      "draw 3329\n",
      "dirti 3330\n",
      "blame 3331\n",
      "idiot 3332\n",
      "yay 3333\n",
      "scalpel 3334\n",
      "seat 3335\n",
      "terrificli 3336\n",
      "literatur 3337\n",
      "dub 3338\n",
      "gloomi 3339\n",
      "dreari 3340\n",
      "forth 3341\n",
      "fabul 3342\n",
      "3000 3343\n",
      "brink 3344\n",
      "mst 3345\n",
      "3k 3346\n",
      "filmfour 3347\n",
      "sci 3348\n",
      "fi 3349\n",
      "manchurian 3350\n",
      "candid 3351\n",
      "cypher 3352\n",
      "paranoia 3353\n",
      "serv 3354\n",
      "morgan 3355\n",
      "sullivan 3356\n",
      "suburban 3357\n",
      "digicorp 3358\n",
      "speech 3359\n",
      "interrupt 3360\n",
      "rita 3361\n",
      "stark 3362\n",
      "blend 3363\n",
      "apocalypt 3364\n",
      "affili 3365\n",
      "clockwork 3366\n",
      "orang 3367\n",
      "effici 3368\n",
      "ditch 3369\n",
      "stylish 3370\n",
      "jeremi 3371\n",
      "northam 3372\n",
      "luci 3373\n",
      "lui 3374\n",
      "quiet 3375\n",
      "dishearten 3376\n",
      "centr 3377\n",
      "inconsist 3378\n",
      "lar 3379\n",
      "von 3380\n",
      "trier 3381\n",
      "backward 3382\n",
      "forgotten 3383\n",
      "postwar 3384\n",
      "nightmarish 3385\n",
      "kessler 3386\n",
      "german 3387\n",
      "descent 3388\n",
      "oblig 3389\n",
      "restor 3390\n",
      "zentropa 3391\n",
      "railway 3392\n",
      "attend 3393\n",
      "passeng 3394\n",
      "shoe 3395\n",
      "polish 3396\n",
      "sole 3397\n",
      "ensu 3398\n",
      "allus 3399\n",
      "fanatic 3400\n",
      "adher 3401\n",
      "allegori 3402\n",
      "trial 3403\n",
      "tribul 3404\n",
      "dash 3405\n",
      "carriag 3406\n",
      "starv 3407\n",
      "auschwitz 3408\n",
      "fleet 3409\n",
      "unconnect 3410\n",
      "urmitz 3411\n",
      "jump 3412\n",
      "parcel 3413\n",
      "bomb 3414\n",
      "stander 3415\n",
      "undersid 3416\n",
      "grass 3417\n",
      "river 3418\n",
      "bank 3419\n",
      "build 3420\n",
      "giant 3421\n",
      "bridg 3422\n",
      "breakneck 3423\n",
      "board 3424\n",
      "analys 3425\n",
      "ridicul 3426\n",
      "row 3427\n",
      "cup 3428\n",
      "hang 3429\n",
      "rattl 3430\n",
      "sway 3431\n",
      "acclaim 3432\n",
      "playwright 3433\n",
      "harl 3434\n",
      "broadcast 3435\n",
      "transplant 3436\n",
      "keven 3437\n",
      "kline 3438\n",
      "glass 3439\n",
      "hilari 3440\n",
      "page 3441\n",
      "salli 3442\n",
      "elizabeth 3443\n",
      "shue 3444\n",
      "cathi 3445\n",
      "moriarti 3446\n",
      "downey 3447\n",
      "teri 3448\n",
      "hatcher 3449\n",
      "garri 3450\n",
      "marshal 3451\n",
      "kathi 3452\n",
      "najimi 3453\n",
      "click 3454\n",
      "slip 3455\n",
      "crack 3456\n",
      "p 3457\n",
      "silvestri 3458\n",
      "bonu 3459\n",
      "flamboy 3460\n",
      "melodramat 3461\n",
      "andrea 3462\n",
      "bianchi 3463\n",
      "legendari 3464\n",
      "lucio 3465\n",
      "fulci 3466\n",
      "mix 3467\n",
      "fest 3468\n",
      "slasher 3469\n",
      "stay 3470\n",
      "awak 3471\n",
      "hooker 3472\n",
      "slaughter 3473\n",
      "road 3474\n",
      "ax 3475\n",
      "wield 3476\n",
      "unfortuanitli 3477\n",
      "30 3478\n",
      "unansw 3479\n",
      "floor 3480\n",
      "freak 3481\n",
      "spoiler 3482\n",
      "jed 3483\n",
      "spinster 3484\n",
      "kate 3485\n",
      "mcdowel 3486\n",
      "wrongli 3487\n",
      "catti 3488\n",
      "en 3489\n",
      "flagrant 3490\n",
      "delicto 3491\n",
      "throw 3492\n",
      "reluctantli 3493\n",
      "crush 3494\n",
      "submit 3495\n",
      "frothi 3496\n",
      "fizz 3497\n",
      "everybodi 3498\n",
      "gal 3499\n",
      "destruct 3500\n",
      "ugh 3501\n",
      "ursula 3502\n",
      "andress 3503\n",
      "offici 3504\n",
      "benito 3505\n",
      "varotto 3506\n",
      "duilio 3507\n",
      "del 3508\n",
      "prete 3509\n",
      "ostens 3510\n",
      "nurs 3511\n",
      "widow 3512\n",
      "leonida 3513\n",
      "bottacin 3514\n",
      "mario 3515\n",
      "piso 3516\n",
      "heir 3517\n",
      "businessmen 3518\n",
      "entrepreneur 3519\n",
      "kitch 3520\n",
      "palanc 3521\n",
      "ulterior 3522\n",
      "motiv 3523\n",
      "derail 3524\n",
      "grow 3525\n",
      "iron 3526\n",
      "erot 3527\n",
      "nude 3528\n",
      "jole 3529\n",
      "malevol 3530\n",
      "heiress 3531\n",
      "luciana 3532\n",
      "paluzzi 3533\n",
      "noteworthi 3534\n",
      "continent 3535\n",
      "bond 3536\n",
      "thunderbal 3537\n",
      "fiona 3538\n",
      "volp 3539\n",
      "femm 3540\n",
      "elegantli 3541\n",
      "adon 3542\n",
      "patient 3543\n",
      "timer 3544\n",
      "fling 3545\n",
      "invect 3546\n",
      "mildli 3547\n",
      "panti 3548\n",
      "swim 3549\n",
      "estat 3550\n",
      "client 3551\n",
      "wherev 3552\n",
      "hey 3553\n",
      "wine 3554\n",
      "cellar 3555\n",
      "chase 3556\n",
      "sensuou 3557\n",
      "compact 3558\n",
      "77 3559\n",
      "ferrari 3560\n",
      "crash 3561\n",
      "eddi 3562\n",
      "griffin 3563\n",
      "shoot 3564\n",
      "paycheck 3565\n",
      "misogynist 3566\n",
      "rise 3567\n",
      "swimsuit 3568\n",
      "cost 3569\n",
      "clarifi 3570\n",
      "user 3571\n",
      "appal 3572\n",
      "hyper 3573\n",
      "ned 3574\n",
      "whimsic 3575\n",
      "refreshingli 3576\n",
      "polar 3577\n",
      "mass 3578\n",
      "lay 3579\n",
      "morbid 3580\n",
      "shelf 3581\n",
      "afi 3582\n",
      "dalla 3583\n",
      "haa 3584\n",
      "brick 3585\n",
      "alpha 3586\n",
      "hollywoodland 3587\n",
      "adam 3588\n",
      "snow 3589\n",
      "confidenti 3590\n",
      "matt 3591\n",
      "bissonnett 3592\n",
      "marri 3593\n",
      "infidel 3594\n",
      "brak 3595\n",
      "sgcc 3596\n",
      "zorak 3597\n",
      "homework 3598\n",
      "goof 3599\n",
      "thunderclees 3600\n",
      "agrument 3601\n",
      "eaten 3602\n",
      "worm 3603\n",
      "bulli 3604\n",
      "beaten 3605\n",
      "blast 3606\n",
      "zap 3607\n",
      "enthral 3608\n",
      "unmiss 3609\n",
      "coffe 3610\n",
      "adrenalin 3611\n",
      "cudo 3612\n",
      "endemol 3613\n",
      "kidney 3614\n",
      "hoax 3615\n",
      "donat 3616\n",
      "previou 3617\n",
      "swept 3618\n",
      "pussi 3619\n",
      "trust 3620\n",
      "internet 3621\n",
      "buzz 3622\n",
      "stinker 3623\n",
      "trepid 3624\n",
      "mainli 3625\n",
      "snappi 3626\n",
      "checklist 3627\n",
      "matrix 3628\n",
      "vanilla 3629\n",
      "frustrat 3630\n",
      "stoog 3631\n",
      "christin 3632\n",
      "mcintyr 3633\n",
      "shemp 3634\n",
      "autumn 3635\n",
      "semit 3636\n",
      "slightest 3637\n",
      "notion 3638\n",
      "mise 3639\n",
      "vari 3640\n",
      "austrian 3641\n",
      "han 3642\n",
      "moser 3643\n",
      "asylum 3644\n",
      "da 3645\n",
      "kabinett 3646\n",
      "dr 3647\n",
      "caligari 3648\n",
      "maker 3649\n",
      "filmmuseum 3650\n",
      "nov 3651\n",
      "rave 3652\n",
      "arnold 3653\n",
      "span 3654\n",
      "predat 3655\n",
      "recal 3656\n",
      "occas 3657\n",
      "futurist 3658\n",
      "2017 3659\n",
      "snicker 3660\n",
      "haircut 3661\n",
      "monitor 3662\n",
      "panel 3663\n",
      "hdtv 3664\n",
      "intro 3665\n",
      "storylin 3666\n",
      "credit 3667\n",
      "elect 3668\n",
      "governor 3669\n",
      "california 3670\n",
      "buzzsaw 3671\n",
      "dynamo 3672\n",
      "firebal 3673\n",
      "subzero 3674\n",
      "stalker 3675\n",
      "overkil 3676\n",
      "quick 3677\n",
      "overplay 3678\n",
      "documentari 3679\n",
      "breathtak 3680\n",
      "landscap 3681\n",
      "growth 3682\n",
      "china 3683\n",
      "edward 3684\n",
      "burtynski 3685\n",
      "educ 3686\n",
      "econom 3687\n",
      "extern 3688\n",
      "paid 3689\n",
      "citizen 3690\n",
      "lifestyl 3691\n",
      "challeng 3692\n",
      "pressur 3693\n",
      "8 3694\n",
      "antwerp 3695\n",
      "eight 3696\n",
      "hennessey 3697\n",
      "kerri 3698\n",
      "bridget 3699\n",
      "teenag 3700\n",
      "cate 3701\n",
      "disciplin 3702\n",
      "sparkl 3703\n",
      "programm 3704\n",
      "relev 3705\n",
      "rori 3706\n",
      "offspr 3707\n",
      "granddad 3708\n",
      "j 3709\n",
      "arriv 3710\n",
      "newer 3711\n",
      "irrit 3712\n",
      "kentucki 3713\n",
      "mellisa 3714\n",
      "joan 3715\n",
      "hart 3716\n",
      "louisvil 3717\n",
      "neighborhood 3718\n",
      "filthi 3719\n",
      "derbi 3720\n",
      "disappear 3721\n",
      "sick 3722\n",
      "cell 3723\n",
      "mate 3724\n",
      "bait 3725\n",
      "partner 3726\n",
      "implant 3727\n",
      "jaw 3728\n",
      "knowledg 3729\n",
      "spi 3730\n",
      "state 3731\n",
      "fangoria 3732\n",
      "magazin 3733\n",
      "amateurishli 3734\n",
      "fay 3735\n",
      "dunaway 3736\n",
      "joy 3737\n",
      "gina 3738\n",
      "phillip 3739\n",
      "raven 3740\n",
      "marzio 3741\n",
      "thick 3742\n",
      "argentinean 3743\n",
      "nichola 3744\n",
      "ultra 3745\n",
      "cohen 3746\n",
      "media 3747\n",
      "raunchi 3748\n",
      "concentr 3749\n",
      "dissect 3750\n",
      "pursuit 3751\n",
      "passion 3752\n",
      "tradit 3753\n",
      "footstep 3754\n",
      "captur 3755\n",
      "convers 3756\n",
      "holt 3757\n",
      "zori 3758\n",
      "barber 3759\n",
      "jonathan 3760\n",
      "abraham 3761\n",
      "unrel 3762\n",
      "obsess 3763\n",
      "foil 3764\n",
      "judah 3765\n",
      "domk 3766\n",
      "amanda 3767\n",
      "peet 3768\n",
      "underneath 3769\n",
      "lurk 3770\n",
      "sugar 3771\n",
      "coat 3772\n",
      "methink 3773\n",
      "quo 3774\n",
      "vadi 3775\n",
      "meander 3776\n",
      "aesthet 3777\n",
      "subtleti 3778\n",
      "enigma 3779\n",
      "taylor 3780\n",
      "poland 3781\n",
      "decad 3782\n",
      "declin 3783\n",
      "pagan 3784\n",
      "infant 3785\n",
      "brandauer 3786\n",
      "forrest 3787\n",
      "syudov 3788\n",
      "petroniu 3789\n",
      "accumul 3790\n",
      "essenc 3791\n",
      "imho 3792\n",
      "moder 3793\n",
      "ilk 3794\n",
      "seagal 3795\n",
      "willi 3796\n",
      "intellectu 3797\n",
      "capac 3798\n",
      "sympathis 3799\n",
      "comprehend 3800\n",
      "compassion 3801\n",
      "le 3802\n",
      "huiti 3803\n",
      "jour 3804\n",
      "jaco 3805\n",
      "dormael 3806\n",
      "simplist 3807\n",
      "defi 3808\n",
      "encapsul 3809\n",
      "georg 3810\n",
      "frighten 3811\n",
      "down 3812\n",
      "syndrom 3813\n",
      "tear 3814\n",
      "jerker 3815\n",
      "compass 3816\n",
      "warmth 3817\n",
      "bump 3818\n",
      "prioriti 3819\n",
      "melt 3820\n",
      "friendship 3821\n",
      "profound 3822\n",
      "cuasi 3823\n",
      "surrealist 3824\n",
      "phantasmagor 3825\n",
      "bu 3826\n",
      "drive 3827\n",
      "detract 3828\n",
      "bear 3829\n",
      "majest 3830\n",
      "rain 3831\n",
      "veneer 3832\n",
      "renown 3833\n",
      "pascal 3834\n",
      "duquenn 3835\n",
      "daniel 3836\n",
      "auteuil 3837\n",
      "strain 3838\n",
      "magnifiqu 3839\n",
      "chapeau 3840\n",
      "ferrer 3841\n",
      "claudett 3842\n",
      "colbert 3843\n",
      "ellen 3844\n",
      "ewe 3845\n",
      "egg 3846\n",
      "47 3847\n",
      "mclean 3848\n",
      "bulg 3849\n",
      "65 3850\n",
      "aid 3851\n",
      "nutti 3852\n",
      "audrey 3853\n",
      "hepburn 3854\n",
      "inspector 3855\n",
      "hollaway 3856\n",
      "solid 3857\n",
      "bennett 3858\n",
      "thespian 3859\n",
      "uncov 3860\n",
      "mundan 3861\n",
      "method 3862\n",
      "charl 3863\n",
      "hillyer 3864\n",
      "denholm 3865\n",
      "elliott 3866\n",
      "fiend 3867\n",
      "latest 3868\n",
      "nonetheless 3869\n",
      "dilli 3870\n",
      "poignant 3871\n",
      "anecdot 3872\n",
      "waxwork 3873\n",
      "grayson 3874\n",
      "cush 3875\n",
      "lonesom 3876\n",
      "nevil 3877\n",
      "splendid 3878\n",
      "joss 3879\n",
      "ackland 3880\n",
      "infatu 3881\n",
      "wax 3882\n",
      "lethal 3883\n",
      "murderess 3884\n",
      "vignett 3885\n",
      "reserv 3886\n",
      "reid 3887\n",
      "semi 3888\n",
      "nanni 3889\n",
      "norton 3890\n",
      "nyre 3891\n",
      "seemingli 3892\n",
      "spooki 3893\n",
      "unnerv 3894\n",
      "ador 3895\n",
      "chloe 3896\n",
      "frank 3897\n",
      "substanti 3898\n",
      "boost 3899\n",
      "seren 3900\n",
      "yarn 3901\n",
      "cloak 3902\n",
      "pompou 3903\n",
      "henderson 3904\n",
      "delight 3905\n",
      "essay 3906\n",
      "haughti 3907\n",
      "hilt 3908\n",
      "jon 3909\n",
      "pertwe 3910\n",
      "transform 3911\n",
      "whenev 3912\n",
      "item 3913\n",
      "benefit 3914\n",
      "awesom 3915\n",
      "pulchritudin 3916\n",
      "ingrid 3917\n",
      "pitt 3918\n",
      "entic 3919\n",
      "vampiress 3920\n",
      "carla 3921\n",
      "duffel 3922\n",
      "delici 3923\n",
      "macabr 3924\n",
      "scribe 3925\n",
      "bloch 3926\n",
      "suitabl 3927\n",
      "eeri 3928\n",
      "parslow 3929\n",
      "crisp 3930\n",
      "shudderi 3931\n",
      "omnibu 3932\n",
      "fare 3933\n",
      "rewir 3934\n",
      "beforehand 3935\n",
      "alien 3936\n",
      "roswel 3937\n",
      "mexico 3938\n",
      "contact 3939\n",
      "andromeda 3940\n",
      "classroom 3941\n",
      "stargat 3942\n",
      "boot 3943\n",
      "deriv 3944\n",
      "amp 3945\n",
      "nope 3946\n",
      "blink 3947\n",
      "teas 3948\n",
      "spader 3949\n",
      "causal 3950\n",
      "shower 3951\n",
      "delect 3952\n",
      "lesli 3953\n",
      "stefanson 3954\n",
      "join 3955\n",
      "aggh 3956\n",
      "liven 3957\n",
      "bath 3958\n",
      "antarctica 3959\n",
      "artific 3960\n",
      "share 3961\n",
      "inquir 3962\n",
      "corn 3963\n",
      "motif 3964\n",
      "pinhead 3965\n",
      "hellrais 3966\n",
      "leprechaun 3967\n",
      "slay 3968\n",
      "nubil 3969\n",
      "cornfield 3970\n",
      "logan 3971\n",
      "stalk 3972\n",
      "aliv 3973\n",
      "coven 3974\n",
      "janin 3975\n",
      "eser 3976\n",
      "antarct 3977\n",
      "beachwear 3978\n",
      "slog 3979\n",
      "carrey 3980\n",
      "bruce 3981\n",
      "nolan 3982\n",
      "freeman 3983\n",
      "aniston 3984\n",
      "subpoint 3985\n",
      "glad 3986\n",
      "som 3987\n",
      "himmelen 3988\n",
      "divin 3989\n",
      "deepli 3990\n",
      "regist 3991\n",
      "octav 3992\n",
      "mikael 3993\n",
      "nyqvist 3994\n",
      "charismat 3995\n",
      "inner 3996\n",
      "mesmeris 3997\n",
      "inclus 3998\n",
      "archetyp 3999\n",
      "messiah 4000\n",
      "victim 4001\n",
      "awaken 4002\n",
      "fond 4003\n",
      "thoroughli 4004\n",
      "choir 4005\n",
      "breathtakingli 4006\n",
      "inform 4007\n",
      "policeman 4008\n",
      "cooper 4009\n",
      "detect 4010\n",
      "baptism 4011\n",
      "bruno 4012\n",
      "godfath 4013\n",
      "dishevel 4014\n",
      "rite 4015\n",
      "eileen 4016\n",
      "backyard 4017\n",
      "respond 4018\n",
      "willingli 4019\n",
      "gossip 4020\n",
      "endear 4021\n",
      "crimin 4022\n",
      "scum 4023\n",
      "bag 4024\n",
      "haul 4025\n",
      "homosexu 4026\n",
      "queer 4027\n",
      "mccree 4028\n",
      "jurisdict 4029\n",
      "petit 4030\n",
      "offic 4031\n",
      "closet 4032\n",
      "fellow 4033\n",
      "account 4034\n",
      "sponsor 4035\n",
      "dare 4036\n",
      "jeannot 4037\n",
      "szwarc 4038\n",
      "thorni 4039\n",
      "sensation 4040\n",
      "chad 4041\n",
      "everett 4042\n",
      "target 4043\n",
      "shane 4044\n",
      "johnson 4045\n",
      "contribut 4046\n",
      "nick 4047\n",
      "vera 4048\n",
      "basketbal 4049\n",
      "lamer 4050\n",
      "borrow 4051\n",
      "feedback 4052\n",
      "possibl 4053\n",
      "pot 4054\n",
      "legal 4055\n",
      "franc 4056\n",
      "lame 4057\n",
      "hall 4058\n",
      "reno 4059\n",
      "descend 4060\n",
      "gag 4061\n",
      "ripe 4062\n",
      "socket 4063\n",
      "tortur 4064\n",
      "drag 4065\n",
      "coffin 4066\n",
      "corps 4067\n",
      "squar 4068\n",
      "degrad 4069\n",
      "legaci 4070\n",
      "spruce 4071\n",
      "goos 4072\n",
      "princip 4073\n",
      "necklac 4074\n",
      "daili 4075\n",
      "sake 4076\n",
      "101 4077\n",
      "pile 4078\n",
      "unnatur 4079\n",
      "miseri 4080\n",
      "howard 4081\n",
      "duck 4082\n",
      "silent 4083\n",
      "clan 4084\n",
      "pickford 4085\n",
      "coast 4086\n",
      "scotland 4087\n",
      "exterior 4088\n",
      "marblehead 4089\n",
      "massachusett 4090\n",
      "rocki 4091\n",
      "seasid 4092\n",
      "geograph 4093\n",
      "churn 4094\n",
      "hometown 4095\n",
      "grandmoth 4096\n",
      "lizzett 4097\n",
      "woodfin 4098\n",
      "cliff 4099\n",
      "statur 4100\n",
      "chiefton 4101\n",
      "historian 4102\n",
      "unsur 4103\n",
      "site 4104\n",
      "primit 4105\n",
      "natal 4106\n",
      "ward 4107\n",
      "linden 4108\n",
      "distraught 4109\n",
      "palpabl 4110\n",
      "loretta 4111\n",
      "prison 4112\n",
      "sentenc 4113\n",
      "of 4114\n",
      "lecher 4115\n",
      "ms 4116\n",
      "disinvit 4117\n",
      "pregnanc 4118\n",
      "outcom 4119\n",
      "alin 4120\n",
      "macmahon 4121\n",
      "apt 4122\n",
      "thomson 4123\n",
      "exud 4124\n",
      "glenda 4125\n",
      "farrel 4126\n",
      "croon 4127\n",
      "franki 4128\n",
      "drunken 4129\n",
      "lullabi 4130\n",
      "mchugh 4131\n",
      "warner 4132\n",
      "permit 4133\n",
      "modestli 4134\n",
      "panic 4135\n",
      "persuas 4136\n",
      "blob 4137\n",
      "amateurish 4138\n",
      "valley 4139\n",
      "forg 4140\n",
      "pa 4141\n",
      "mismatch 4142\n",
      "lyric 4143\n",
      "bwp 4144\n",
      "excrement 4145\n",
      "meteorit 4146\n",
      "poke 4147\n",
      "doctor 4148\n",
      "slowli 4149\n",
      "blanket 4150\n",
      "gurney 4151\n",
      "120 4152\n",
      "inflat 4153\n",
      "passabl 4154\n",
      "chri 4155\n",
      "sarandon 4156\n",
      "burgess 4157\n",
      "meredith 4158\n",
      "eva 4159\n",
      "beverli 4160\n",
      "angelo 4161\n",
      "mute 4162\n",
      "carradin 4163\n",
      "chair 4164\n",
      "walken 4165\n",
      "tad 4166\n",
      "seventi 4167\n",
      "overproduc 4168\n",
      "spartan 4169\n",
      "makeup 4170\n",
      "higher 4171\n",
      "justifi 4172\n",
      "dariu 4173\n",
      "west 4174\n",
      "weem 4175\n",
      "duchenn 4176\n",
      "muscular 4177\n",
      "dystrophi 4178\n",
      "pimp 4179\n",
      "ride 4180\n",
      "wheelchair 4181\n",
      "sunscreen 4182\n",
      "tribeca 4183\n",
      "athen 4184\n",
      "greec 4185\n",
      "snowbal 4186\n",
      "orlean 4187\n",
      "atlanta 4188\n",
      "palm 4189\n",
      "fl 4190\n",
      "georgia 4191\n",
      "ticket 4192\n",
      "sold 4193\n",
      "dgw 4194\n",
      "kish 4195\n",
      "visionari 4196\n",
      "gilliam 4197\n",
      "brazil 4198\n",
      "outdon 4199\n",
      "frenet 4200\n",
      "stow 4201\n",
      "leagu 4202\n",
      "layer 4203\n",
      "magnific 4204\n",
      "suspici 4205\n",
      "quantum 4206\n",
      "choke 4207\n",
      "unabl 4208\n",
      "shovel 4209\n",
      "clincher 4210\n",
      "revel 4211\n",
      "looni 4212\n",
      "ramtha 4213\n",
      "applecart 4214\n",
      "surrend 4215\n",
      "dorothi 4216\n",
      "dian 4217\n",
      "keaton 4218\n",
      "dearth 4219\n",
      "twenti 4220\n",
      "impos 4221\n",
      "gradual 4222\n",
      "teleplay 4223\n",
      "coars 4224\n",
      "dim 4225\n",
      "phoni 4226\n",
      "tidi 4227\n",
      "scenario 4228\n",
      "bungalow 4229\n",
      "resid 4230\n",
      "muscl 4231\n",
      "shirt 4232\n",
      "wizard 4233\n",
      "oz 4234\n",
      "kane 4235\n",
      "personag 4236\n",
      "kooki 4237\n",
      "warmli 4238\n",
      "flexibl 4239\n",
      "flaki 4240\n",
      "craze 4241\n",
      "harpi 4242\n",
      "discoveri 4243\n",
      "odyssey 4244\n",
      "nervou 4245\n",
      "overag 4246\n",
      "caveat 4247\n",
      "intrins 4248\n",
      "downer 4249\n",
      "muddl 4250\n",
      "derang 4251\n",
      "wind 4252\n",
      "lurid 4253\n",
      "glossi 4254\n",
      "dysfunct 4255\n",
      "texa 4256\n",
      "oil 4257\n",
      "deep 4258\n",
      "social 4259\n",
      "dougla 4260\n",
      "sirk 4261\n",
      "univer 4262\n",
      "sudser 4263\n",
      "stack 4264\n",
      "lauren 4265\n",
      "bacal 4266\n",
      "pal 4267\n",
      "hudson 4268\n",
      "pregnant 4269\n",
      "firework 4270\n",
      "malon 4271\n",
      "portrayl 4272\n",
      "nympho 4273\n",
      "shun 4274\n",
      "74 4275\n",
      "asian 4276\n",
      "uninvit 4277\n",
      "aka 4278\n",
      "inyong 4279\n",
      "shiktak 4280\n",
      "su 4281\n",
      "leon 4282\n",
      "calib 4283\n",
      "balanc 4284\n",
      "hick 4285\n",
      "musician 4286\n",
      "imperson 4287\n",
      "meat 4288\n",
      "loaf 4289\n",
      "deborah 4290\n",
      "carney 4291\n",
      "roadi 4292\n",
      "goal 4293\n",
      "groupi 4294\n",
      "plausibl 4295\n",
      "certain 4296\n",
      "gailard 4297\n",
      "sartain 4298\n",
      "promin 4299\n",
      "exce 4300\n",
      "neil 4301\n",
      "hamilton 4302\n",
      "cavanagh 4303\n",
      "tarzan 4304\n",
      "jungl 4305\n",
      "escarp 4306\n",
      "ivori 4307\n",
      "mine 4308\n",
      "eleph 4309\n",
      "graveyard 4310\n",
      "ape 4311\n",
      "parti 4312\n",
      "nativ 4313\n",
      "tribe 4314\n",
      "feed 4315\n",
      "cruelti 4316\n",
      "gotta 4317\n",
      "13 4318\n",
      "strangelov 4319\n",
      "expens 4320\n",
      "hollwood 4321\n",
      "mock 4322\n",
      "ha 4323\n",
      "gordon 4324\n",
      "vonnegut 4325\n",
      "merciless 4326\n",
      "quirki 4327\n",
      "nolt 4328\n",
      "goodman 4329\n",
      "arkin 4330\n",
      "sheryl 4331\n",
      "laura 4332\n",
      "maddi 4333\n",
      "pierc 4334\n",
      "brosnan 4335\n",
      "recogn 4336\n",
      "gamer 4337\n",
      "gentlemen 4338\n",
      "partli 4339\n",
      "stiff 4340\n",
      "caption 4341\n",
      "yea 4342\n",
      "execut 4343\n",
      "ohhh 4344\n",
      "ohhhhh 4345\n",
      "breed 4346\n",
      "ohhhh 4347\n",
      "prejudic 4348\n",
      "jeez 4349\n",
      "whiney 4350\n",
      "revisionist 4351\n",
      "analyz 4352\n",
      "loud 4353\n",
      "meryl 4354\n",
      "steep 4355\n",
      "tediou 4356\n",
      "tiresom 4357\n",
      "coburn 4358\n",
      "hackman 4359\n",
      "bergen 4360\n",
      "19th 4361\n",
      "desert 4362\n",
      "dentisti 4363\n",
      "mexican 4364\n",
      "creep 4365\n",
      "stapl 4366\n",
      "luke 4367\n",
      "triniti 4368\n",
      "hors 4369\n",
      "deader 4370\n",
      "spacecamp 4371\n",
      "ruth 4372\n",
      "joaquin 4373\n",
      "phoenix 4374\n",
      "he 4375\n",
      "emperor 4376\n",
      "gladiat 4377\n",
      "comm 4378\n",
      "artifici 4379\n",
      "qe2 4380\n",
      "holodeck 4381\n",
      "coaltrain 4382\n",
      "gate 4383\n",
      "ki 4384\n",
      "bunker 4385\n",
      "faster 4386\n",
      "23rd 4387\n",
      "command 4388\n",
      "gameboy 4389\n",
      "gauntlet 4390\n",
      "ii 4391\n",
      "thiev 4392\n",
      "jfk 4393\n",
      "2023 4394\n",
      "greatli 4395\n",
      "leadership 4396\n",
      "presid 4397\n",
      "drastic 4398\n",
      "global 4399\n",
      "boss 4400\n",
      "ai 4401\n",
      "indoor 4402\n",
      "capshaw 4403\n",
      "wow 4404\n",
      "freedom 4405\n",
      "delet 4406\n",
      "execr 4407\n",
      "jewison 4408\n",
      "et 4409\n",
      "al 4410\n",
      "apallingli 4411\n",
      "pretenti 4412\n",
      "banal 4413\n",
      "trough 4414\n",
      "lap 4415\n",
      "worthwhil 4416\n",
      "embarass 4417\n",
      "premis 4418\n",
      "thoma 4419\n",
      "archer 4420\n",
      "traumat 4421\n",
      "stress 4422\n",
      "disord 4423\n",
      "therapi 4424\n",
      "strap 4425\n",
      "culprit 4426\n",
      "implement 4427\n",
      "retribut 4428\n",
      "drill 4429\n",
      "bat 4430\n",
      "sledgehamm 4431\n",
      "torch 4432\n",
      "crime 4433\n",
      "proclaim 4434\n",
      "abc 4435\n",
      "summar 4436\n",
      "survivor 4437\n",
      "strand 4438\n",
      "disservic 4439\n",
      "unfound 4440\n",
      "season 4441\n",
      "fortun 4442\n",
      "size 4443\n",
      "grandeur 4444\n",
      "biblic 4445\n",
      "diminish 4446\n",
      "compel 4447\n",
      "gilbert 4448\n",
      "barnett 4449\n",
      "absolom 4450\n",
      "susan 4451\n",
      "hayward 4452\n",
      "bathsheba 4453\n",
      "seductress 4454\n",
      "sympath 4455\n",
      "raymond 4456\n",
      "massey 4457\n",
      "prophet 4458\n",
      "alfr 4459\n",
      "newman 4460\n",
      "robe 4461\n",
      "shamroy 4462\n",
      "intimaci 4463\n",
      "henri 4464\n",
      "wyler 4465\n",
      "lose 4466\n",
      "tread 4467\n",
      "hokey 4468\n",
      "paraphras 4469\n",
      "embark 4470\n",
      "belong 4471\n",
      "gregori 4472\n",
      "peck 4473\n",
      "ruthlessli 4474\n",
      "psalm 4475\n",
      "trace 4476\n",
      "explan 4477\n",
      "bourn 4478\n",
      "vengeanc 4479\n",
      "elabor 4480\n",
      "themat 4481\n",
      "hollywoodian 4482\n",
      "supremati 4483\n",
      "invinc 4484\n",
      "frontier 4485\n",
      "24 4486\n",
      "belief 4487\n",
      "paranoiac 4488\n",
      "besid 4489\n",
      "flawless 4490\n",
      "feminin 4491\n",
      "nicki 4492\n",
      "parson 4493\n",
      "useless 4494\n",
      "unorigin 4495\n",
      "dialog 4496\n",
      "ultimatum 4497\n",
      "waterloo 4498\n",
      "tanger 4499\n",
      "exagger 4500\n",
      "tsui 4501\n",
      "hark 4502\n",
      "contemporari 4503\n",
      "jade 4504\n",
      "ick 4505\n",
      "beastial 4506\n",
      "explicit 4507\n",
      "footag 4508\n",
      "shave 4509\n",
      "butler 4510\n",
      "victorian 4511\n",
      "ravag 4512\n",
      "beast 4513\n",
      "forest 4514\n",
      "heh 4515\n",
      "wrap 4516\n",
      "plotless 4517\n",
      "passionless 4518\n",
      "sultri 4519\n",
      "dafo 4520\n",
      "colagrand 4521\n",
      "valium 4522\n",
      "tampon 4523\n",
      "willem 4524\n",
      "vagina 4525\n",
      "restaur 4526\n",
      "waiter 4527\n",
      "sleepi 4528\n",
      "deconstruct 4529\n",
      "jambalaya 4530\n",
      "separ 4531\n",
      "simmer 4532\n",
      "spous 4533\n",
      "violin 4534\n",
      "weirdo 4535\n",
      "caretak 4536\n",
      "auto 4537\n",
      "mesh 4538\n",
      "coher 4539\n",
      "artsi 4540\n",
      "prospect 4541\n",
      "renter 4542\n",
      "hopkin 4543\n",
      "steam 4544\n",
      "quentin 4545\n",
      "fastest 4546\n",
      "ah 4547\n",
      "scifi 4548\n",
      "speechless 4549\n",
      "tonight 4550\n",
      "rope 4551\n",
      "nine 4552\n",
      "niec 4553\n",
      "rusti 4554\n",
      "clamp 4555\n",
      "tool 4556\n",
      "tighten 4557\n",
      "burn 4558\n",
      "pit 4559\n",
      "mike 4560\n",
      "nicol 4561\n",
      "eggert 4562\n",
      "pervert 4563\n",
      "baywatch 4564\n",
      "breast 4565\n",
      "paprika 4566\n",
      "cinnamon 4567\n",
      "alex 4568\n",
      "menes 4569\n",
      "ukranian 4570\n",
      "biko 4571\n",
      "activist 4572\n",
      "africa 4573\n",
      "gandhi 4574\n",
      "coloni 4575\n",
      "india 4576\n",
      "attenborough 4577\n",
      "apartheid 4578\n",
      "donald 4579\n",
      "liber 4580\n",
      "newspap 4581\n",
      "editor 4582\n",
      "jar 4583\n",
      "tortuou 4584\n",
      "kevin 4585\n",
      "flee 4586\n",
      "publish 4587\n",
      "penelop 4588\n",
      "wilton 4589\n",
      "vain 4590\n",
      "degener 4591\n",
      "border 4592\n",
      "denzel 4593\n",
      "washington 4594\n",
      "topic 4595\n",
      "befriend 4596\n",
      "conduc 4597\n",
      "pg 4598\n",
      "sanit 4599\n",
      "sg 4600\n",
      "spin 4601\n",
      "1994 4602\n",
      "retir 4603\n",
      "neill 4604\n",
      "companion 4605\n",
      "jackson 4606\n",
      "samantha 4607\n",
      "carter 4608\n",
      "teal 4609\n",
      "warrior 4610\n",
      "gouald 4611\n",
      "parasit 4612\n",
      "insert 4613\n",
      "deed 4614\n",
      "massiv 4615\n",
      "jaffa 4616\n",
      "exploratori 4617\n",
      "wormhol 4618\n",
      "instantli 4619\n",
      "transport 4620\n",
      "knit 4621\n",
      "75 4622\n",
      "instanc 4623\n",
      "emphasi 4624\n",
      "notch 4625\n",
      "duk 4626\n",
      "testimoni 4627\n",
      "summer 4628\n",
      "heartbreak 4629\n",
      "stagger 4630\n",
      "guard 4631\n",
      "squirm 4632\n",
      "nonchal 4633\n",
      "prostitut 4634\n",
      "patent 4635\n",
      "samaritan 4636\n",
      "float 4637\n",
      "placidli 4638\n",
      "interview 4639\n",
      "putresc 4640\n",
      "potti 4641\n",
      "covet 4642\n",
      "merchant 4643\n",
      "swing 4644\n",
      "nauseatingli 4645\n",
      "bestial 4646\n",
      "carnal 4647\n",
      "mindless 4648\n",
      "utterli 4649\n",
      "confound 4650\n",
      "fabl 4651\n",
      "parabl 4652\n",
      "greater 4653\n",
      "unfamiliar 4654\n",
      "audit 4655\n",
      "takashi 4656\n",
      "miik 4657\n",
      "satisfact 4658\n",
      "astonishingli 4659\n",
      "seo 4660\n",
      "jung 4661\n",
      "yoo 4662\n",
      "suk 4663\n",
      "isl 4664\n",
      "pornograph 4665\n",
      "senselessli 4666\n",
      "sadist 4667\n",
      "masochist 4668\n",
      "behav 4669\n",
      "impuls 4670\n",
      "amidst 4671\n",
      "korean 4672\n",
      "quirk 4673\n",
      "shroud 4674\n",
      "boat 4675\n",
      "terrain 4676\n",
      "romper 4677\n",
      "pleasantli 4678\n",
      "motorbik 4679\n",
      "downright 4680\n",
      "heap 4681\n",
      "disco 4682\n",
      "evok 4683\n",
      "novelti 4684\n",
      "neither 4685\n",
      "humbl 4686\n",
      "mediocr 4687\n",
      "angi 4688\n",
      "harmon 4689\n",
      "bitchi 4690\n",
      "charli 4691\n",
      "unscath 4692\n",
      "denis 4693\n",
      "hk 4694\n",
      "spars 4695\n",
      "dynasti 4696\n",
      "blight 4697\n",
      "wrangl 4698\n",
      "infern 4699\n",
      "disjoint 4700\n",
      "spoken 4701\n",
      "monologu 4702\n",
      "biehn 4703\n",
      "sammo 4704\n",
      "hung 4705\n",
      "2036 4706\n",
      "kurt 4707\n",
      "todd 4708\n",
      "birth 4709\n",
      "younger 4710\n",
      "injur 4711\n",
      "dump 4712\n",
      "fewer 4713\n",
      "camp 4714\n",
      "civil 4715\n",
      "adjust 4716\n",
      "danger 4717\n",
      "outsmart 4718\n",
      "rambo 4719\n",
      "commando 4720\n",
      "scientist 4721\n",
      "sebastian 4722\n",
      "bacon 4723\n",
      "invis 4724\n",
      "serum 4725\n",
      "gorilla 4726\n",
      "celebr 4727\n",
      "breakthrough 4728\n",
      "backer 4729\n",
      "persuad 4730\n",
      "procedur 4731\n",
      "antidot 4732\n",
      "laboratori 4733\n",
      "morn 4734\n",
      "chocol 4735\n",
      "fizzi 4736\n",
      "w 4737\n",
      "marlow 4738\n",
      "1933 4739\n",
      "owe 4740\n",
      "110 4741\n",
      "molest 4742\n",
      "rape 4743\n",
      "exploit 4744\n",
      "tea 4745\n",
      "climat 4746\n",
      "revers 4747\n",
      "gori 4748\n",
      "spike 4749\n",
      "bust 4750\n",
      "splatter 4751\n",
      "faultless 4752\n",
      "boffin 4753\n",
      "map 4754\n",
      "hose 4755\n",
      "extravaganza 4756\n",
      "meaning 4757\n",
      "nastier 4758\n",
      "darker 4759\n",
      "streak 4760\n",
      "190 4761\n",
      "wallac 4762\n",
      "calvin 4763\n",
      "camazotz 4764\n",
      "madelein 4765\n",
      "engl 4766\n",
      "jare 4767\n",
      "diamond 4768\n",
      "understood 4769\n",
      "minded 4770\n",
      "taught 4771\n",
      "histor 4772\n",
      "inaccur 4773\n",
      "slade 4774\n",
      "idaho 4775\n",
      "colorado 4776\n",
      "territori 4777\n",
      "montana 4778\n",
      "lousi 4779\n",
      "superintend 4780\n",
      "townsmen 4781\n",
      "drunk 4782\n",
      "virginia 4783\n",
      "insur 4784\n",
      "terror 4785\n",
      "vigilant 4786\n",
      "town 4787\n",
      "butch 4788\n",
      "chairman 4789\n",
      "coffeehous 4790\n",
      "activ 4791\n",
      "1972 4792\n",
      "groov 4793\n",
      "tube 4794\n",
      "skit 4795\n",
      "regular 4796\n",
      "basi 4797\n",
      "ala 4798\n",
      "distribut 4799\n",
      "chevi 4800\n",
      "howl 4801\n",
      "vid 4802\n",
      "koko 4803\n",
      "duguay 4804\n",
      "hitler 4805\n",
      "error 4806\n",
      "incom 4807\n",
      "member 4808\n",
      "unstabl 4809\n",
      "politician 4810\n",
      "contamin 4811\n",
      "ton 4812\n",
      "rough 4813\n",
      "inglori 4814\n",
      "bastard 4815\n",
      "inaccuraci 4816\n",
      "spectat 4817\n",
      "via 4818\n",
      "discuss 4819\n",
      "jewish 4820\n",
      "cabaret 4821\n",
      "reaction 4822\n",
      "fritz 4823\n",
      "gerlich 4824\n",
      "rampag 4825\n",
      "cheer 4826\n",
      "jew 4827\n",
      "necess 4828\n",
      "extermin 4829\n",
      "unsatisfactori 4830\n",
      "cash 4831\n",
      "commerci 4832\n",
      "shelbi 4833\n",
      "cobra 4834\n",
      "angelina 4835\n",
      "cage 4836\n",
      "bg 4837\n",
      "superfic 4838\n",
      "brigadoon 4839\n",
      "packag 4840\n",
      "gene 4841\n",
      "minnelli 4842\n",
      "reunit 4843\n",
      "craftsmen 4844\n",
      "collabor 4845\n",
      "cyd 4846\n",
      "chariss 4847\n",
      "dancer 4848\n",
      "lerner 4849\n",
      "loew 4850\n",
      "gigi 4851\n",
      "sanguin 4852\n",
      "innat 4853\n",
      "conservat 4854\n",
      "mgm 4855\n",
      "assign 4856\n",
      "denizen 4857\n",
      "strove 4858\n",
      "distract 4859\n",
      "naiv 4860\n",
      "plaid 4861\n",
      "conjur 4862\n",
      "nostalgia 4863\n",
      "whimsi 4864\n",
      "luster 4865\n",
      "yolanda 4866\n",
      "thief 4867\n",
      "highland 4868\n",
      "exot 4869\n",
      "malcont 4870\n",
      "proceed 4871\n",
      "township 4872\n",
      "arcan 4873\n",
      "provinci 4874\n",
      "attribut 4875\n",
      "pastor 4876\n",
      "sacrif 4877\n",
      "bless 4878\n",
      "refug 4879\n",
      "assur 4880\n",
      "suspicion 4881\n",
      "quasi 4882\n",
      "queasi 4883\n",
      "paradis 4884\n",
      "insultingli 4885\n",
      "patron 4886\n",
      "miscast 4887\n",
      "virgin 4888\n",
      "optimist 4889\n",
      "lucil 4890\n",
      "bremer 4891\n",
      "levant 4892\n",
      "sidekick 4893\n",
      "cigarett 4894\n",
      "aip 4895\n",
      "toss 4896\n",
      "catapult 4897\n",
      "pew 4898\n",
      "preacher 4899\n",
      "drone 4900\n",
      "lego 4901\n",
      "plant 4902\n",
      "renni 4903\n",
      "heal 4904\n",
      "morph 4905\n",
      "jambore 4906\n",
      "evangelist 4907\n",
      "induc 4908\n",
      "babbl 4909\n",
      "53 4910\n",
      "31st 4911\n",
      "context 4912\n",
      "synonym 4913\n",
      "prey 4914\n",
      "bearabl 4915\n",
      "prayer 4916\n",
      "preyer 4917\n",
      "zani 4918\n",
      "sunk 4919\n",
      "boredom 4920\n",
      "saliv 4921\n",
      "rank 4922\n",
      "theolog 4923\n",
      "reinforc 4924\n",
      "safeti 4925\n",
      "circl 4926\n",
      "narnia 4927\n",
      "superstar 4928\n",
      "cerebr 4929\n",
      "religul 4930\n",
      "whitlow 4931\n",
      "fungal 4932\n",
      "maypo 4933\n",
      "stomach 4934\n",
      "maltex 4935\n",
      "wheatena 4936\n",
      "granola 4937\n",
      "slop 4938\n",
      "sherri 4939\n",
      "subgenr 4940\n",
      "ive 4941\n",
      "dinocroc 4942\n",
      "croc 4943\n",
      "repeat 4944\n",
      "longeneck 4945\n",
      "fleshi 4946\n",
      "cough 4947\n",
      "komodo 4948\n",
      "blacksmith 4949\n",
      "byu 4950\n",
      "maiden 4951\n",
      "info 4952\n",
      "dispens 4953\n",
      "melissa 4954\n",
      "thorn 4955\n",
      "despic 4956\n",
      "deprav 4957\n",
      "bound 4958\n",
      "mesmer 4959\n",
      "excurs 4960\n",
      "sickeningli 4961\n",
      "undercurr 4962\n",
      "majorli 4963\n",
      "rob 4964\n",
      "parent 4965\n",
      "awri 4966\n",
      "skewered 4967\n",
      "deck 4968\n",
      "poker 4969\n",
      "tidbit 4970\n",
      "sibl 4971\n",
      "rivalri 4972\n",
      "wrench 4973\n",
      "shakespearean 4974\n",
      "caution 4975\n",
      "altman 4976\n",
      "ensembl 4977\n",
      "bagdad 4978\n",
      "held 4979\n",
      "drawback 4980\n",
      "nowaday 4981\n",
      "eleven 4982\n",
      "succe 4983\n",
      "dusti 4984\n",
      "depart 4985\n",
      "sabu 4986\n",
      "abu 4987\n",
      "conrad 4988\n",
      "veidt 4989\n",
      "jaffar 4990\n",
      "oldi 4991\n",
      "goldi 4992\n",
      "gut 4993\n",
      "someday 4994\n",
      "skeptic 4995\n",
      "gerard 4996\n",
      "depardieu 4997\n",
      "disgust 4998\n",
      "breathless 4999\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use this space to determine the five most frequently appearing words in the training set.\n",
    "for key, value in word_dict.items() :\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `word_dict`\n",
    "\n",
    "Later on when we construct an endpoint which processes a submitted review we will need to make use of the `word_dict` which we have created. As such, we will save it to a file now for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the reviews\n",
    "\n",
    "Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is `500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check to make sure that things are working as intended, check to see what one of the reviews in the training set looks like after having been processeed. Does this look reasonable? What is the length of a review in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 780  459 1900   73 1901 1454   68  272 1227  167  272 1902  982 1394\n",
      "  572 1902 1903 1904  146   28  260  203   77  596    4 1905 1906   91\n",
      "  793  410  430 1907 1908 1189 1909  412  866   91 1259   72   94    4\n",
      "   68  735 1910 1911   53 1912 1910  290  573  701 1913   38 1154  735\n",
      "  272  248    4 1299 1914  319  318 1309    4 1309    4  259  601 1915\n",
      " 1916    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
    "print(train_X[47])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** In the cells above we use the `preprocess_data` and `convert_and_pad_data` methods to process both the training and testing set. Why or why not might this be a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We did not include the testing set in the word dictionary.  We only used the training data to build the dictionary.  Since we are coding words that aren't in the dictionary as infrequent, this could affect the way the testing data gets processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload the data to S3\n",
    "\n",
    "As in the XGBoost notebook, we will need to upload the training dataset to S3 in order for our training code to access it. For now we will save it locally and we will upload to S3 later on.\n",
    "\n",
    "### Save the processed training dataset locally\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form `label`, `length`, `review[500]` where `review[500]` is a sequence of `500` integers representing the words in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the training data\n",
    "\n",
    "\n",
    "Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The cell above uploads the entire contents of our data directory. This includes the `word_dict.pkl` file. This is fortunate as we will need this later on when we create an endpoint that accepts an arbitrary review. For now, we will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that we will need to make sure it gets saved in the model directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and Train the PyTorch Model\n",
    "\n",
    "In the XGBoost notebook we discussed what a model is in the SageMaker framework. In particular, a model comprises three objects\n",
    "\n",
    " - Model Artifacts,\n",
    " - Training Code, and\n",
    " - Inference Code,\n",
    " \n",
    "each of which interact with one another. In the XGBoost example we used training and inference code that was provided by Amazon. Here we will still be using containers provided by Amazon with the added benefit of being able to include our own custom code.\n",
    "\n",
    "We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we have provided the necessary model object in the `model.py` file, inside of the `train` folder. You can see the provided implementation by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        x = x.t()\r\n",
      "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
      "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
      "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
      "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
      "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important takeaway from the implementation provided is that there are three parameters that we may wish to tweak to improve the performance of our model. These are the embedding dimension, the hidden dimension and the size of the vocabulary. We will likely want to make these parameters configurable in the training script so that if we wish to modify them we do not need to modify the script itself. We will see how to do this later on. To start we will write some of the training code in the notebook so that we can more easily diagnose any issues that arise.\n",
    "\n",
    "First we will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as we do not have access to a gpu and the compute instance that we are using is not particularly powerful. However, we can work on a small bit of the data to get a feel for how our training script is behaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Writing the training method\n",
    "\n",
    "Next we need to write the training code itself. This should be very similar to training methods that you have written before to train PyTorch models. We will leave any difficult aspects such as model saving / loading and parameter loading until a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(batch_X)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that we loaded earlier. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6956329822540284\n",
      "Epoch: 2, BCELoss: 0.6840875387191773\n",
      "Epoch: 3, BCELoss: 0.6739113211631775\n",
      "Epoch: 4, BCELoss: 0.6633580088615417\n",
      "Epoch: 5, BCELoss: 0.6519403576850891\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a `requirements.txt` file and install any required Python libraries, after which the training script will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Training the model\n",
    "\n",
    "When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the `train` directory is a file called `train.py` which has been provided and which contains most of the necessary code to train our model. The only thing that is missing is the implementation of the `train()` method which you wrote earlier in this notebook.\n",
    "\n",
    "**TODO**: Copy the `train()` method written above and paste it into the `train/train.py` file where required.\n",
    "\n",
    "The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided `train/train.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-30 00:59:07 Starting - Starting the training job...\n",
      "2019-11-30 00:59:09 Starting - Launching requested ML instances......\n",
      "2019-11-30 01:00:15 Starting - Preparing the instances for training......\n",
      "2019-11-30 01:01:19 Downloading - Downloading input data...\n",
      "2019-11-30 01:02:02 Training - Downloading the training image...\n",
      "2019-11-30 01:02:21 Training - Training image download completed. Training in progress.\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:22,072 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:22,097 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:22,314 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:22,562 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:22,562 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:22,562 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:22,563 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[31mCollecting numpy (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/e9/2561dbfbc05146bffa02167e09b9902e273decb2dc4cd5c43314ede20312/numpy-1.17.4-cp35-cp35m-manylinux1_x86_64.whl (19.8MB)\u001b[0m\n",
      "\u001b[31mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\u001b[0m\n",
      "\u001b[31mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/c8/a55eb6ea11cd7e5ac4bacdf92bac4693b90d3ba79268be16527555e186f0/beautifulsoup4-4.8.1-py3-none-any.whl (101kB)\u001b[0m\n",
      "\u001b[31mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[31mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (1.11.0)\u001b[0m\n",
      "\u001b[31mCollecting soupsieve>=1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/81/94/03c0f04471fc245d08d0a99f7946ac228ca98da4fa75796c507f61e688c2/soupsieve-1.9.5-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[31mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: nltk, train\n",
      "  Running setup.py bdist_wheel for nltk: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-r7f2akgw/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built nltk train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: numpy, pytz, pandas, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[31mSuccessfully installed beautifulsoup4-4.8.1 html5lib-1.0.1 nltk-3.4.5 numpy-1.17.4 pandas-0.24.2 pytz-2019.3 soupsieve-1.9.5 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-11-30 01:02:34,525 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-11-30-00-59-06-446\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"log_level\": 20,\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 200,\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"num_gpus\": 1,\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"module_name\": \"train\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-1-772943077951/sagemaker-pytorch-2019-11-30-00-59-06-446/source/sourcedir.tar.gz\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ]\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-ap-northeast-1-772943077951/sagemaker-pytorch-2019-11-30-00-59-06-446/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2019-11-30-00-59-06-446\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-1-772943077951/sagemaker-pytorch-2019-11-30-00-59-06-446/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cuda.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
      "\u001b[31mEpoch: 1, BCELoss: 0.658062756061554\u001b[0m\n",
      "\u001b[31mEpoch: 2, BCELoss: 0.5717962408552364\u001b[0m\n",
      "\u001b[31mEpoch: 3, BCELoss: 0.526768085299706\u001b[0m\n",
      "\u001b[31mEpoch: 4, BCELoss: 0.45145042575135524\u001b[0m\n",
      "\u001b[31mEpoch: 5, BCELoss: 0.4027098082766241\u001b[0m\n",
      "\u001b[31mEpoch: 6, BCELoss: 0.3664918031011309\u001b[0m\n",
      "\u001b[31mEpoch: 7, BCELoss: 0.3405252579523593\u001b[0m\n",
      "\u001b[31mEpoch: 8, BCELoss: 0.33503770402499605\u001b[0m\n",
      "\u001b[31mEpoch: 9, BCELoss: 0.325288998229163\u001b[0m\n",
      "\u001b[31mEpoch: 10, BCELoss: 0.3019470350474727\u001b[0m\n",
      "\u001b[31m2019-11-30 01:05:38,991 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-11-30 01:05:49 Uploading - Uploading generated training model\n",
      "2019-11-30 01:05:49 Completed - Training job completed\n",
      "Training seconds: 270\n",
      "Billable seconds: 270\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Testing the model\n",
    "\n",
    "As mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly.\n",
    "\n",
    "## Step 6: Deploy the model for testing\n",
    "\n",
    "Now that we have trained our model, we would like to test it to see how it performs. Currently our model takes input of the form `review_length, review[500]` where `review[500]` is a sequence of `500` integers which describe the words present in the review, encoded using `word_dict`. Fortunately for us, SageMaker provides built-in inference code for models with simple inputs such as this.\n",
    "\n",
    "There is one thing that we need to provide, however, and that is a function which loads the saved model. This function must be called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file which we specified as the entry point. In our case the model loading function has been provided and so no changes need to be made.\n",
    "\n",
    "**NOTE**: When the built-in inference code is run it must import the `model_fn()` method from the `train.py` file. This is why the training code is wrapped in a main guard ( ie, `if __name__ == '__main__':` )\n",
    "\n",
    "Since we don't need to change anything in the code that was uploaded during training, we can simply deploy the current model as-is.\n",
    "\n",
    "**NOTE:** When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until *you* shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.\n",
    "\n",
    "In other words **If you are no longer using a deployed endpoint, shut it down!**\n",
    "\n",
    "**TODO:** Deploy the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# TODO: Deploy the trained model\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\n",
    "    print(\"Loading model.\")\n",
    "\n",
    "    # First, load the parameters used to create the model.\n",
    "    model_info = {}\n",
    "    model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "    with open(model_info_path, 'rb') as f:\n",
    "        model_info = torch.load(f)\n",
    "\n",
    "    print(\"model_info: {}\".format(model_info))\n",
    "\n",
    "    # Determine the device and construct the model.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTMClassifier(model_info['embedding_dim'], model_info['hidden_dim'], model_info['vocab_size'])\n",
    "\n",
    "    # Load the stored model parameters.\n",
    "    model_path = os.path.join(model_dir, 'model.pth')\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "\n",
    "    # Load the saved word_dict.\n",
    "    word_dict_path = os.path.join(model_dir, 'word_dict.pkl')\n",
    "    with open(word_dict_path, 'rb') as f:\n",
    "        model.word_dict = pickle.load(f)\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    print(\"Done loading model.\")\n",
    "    return model\n",
    "\n",
    "predictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Use the model for testing\n",
    "\n",
    "Once deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84276"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does this model compare to the XGBoost model you created earlier? Why might these two models perform differently on this dataset? Which do *you* think is better for sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** This model does slightly better.  I'm not sure how XGBoost works but it seems like an RNN is better for predictions in this case.  The memory module might be lending to the accuracy by establishing a kind of relationship approximation between words instead of just looking at all the word occurences as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) More testing\n",
    "\n",
    "We now have a trained model which has been deployed and which we can send processed reviews to and which returns the predicted sentiment. However, ultimately we would like to be able to send our model an unprocessed review. That is, we would like to send the review itself as a string. For example, suppose we wish to send the following review to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we now need to answer is, how do we send this review to our model?\n",
    "\n",
    "Recall in the first section of this notebook we did a bunch of data processing to the IMDb dataset. In particular, we did two specific things to the provided reviews.\n",
    " - Removed any html tags and stemmed the input\n",
    " - Encoded the review as a sequence of integers using `word_dict`\n",
    " \n",
    "In order process the review we will need to repeat these two steps.\n",
    "\n",
    "**TODO**: Using the `review_to_words` and `convert_and_pad` methods from section one, convert `test_review` into a numpy array `test_data` suitable to send to our model. Remember that our model expects input of the form `review_length, review[500]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 448, 302, 36, 30, 34, 962, 92, 519, 3666, 410, 636, 4, 2205, 707, 1, 2516, 2383, 2172, 2709, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Convert test_review into a form usable by the model and save the results in test_data\n",
    "test_words = review_to_words(test_review)\n",
    "test_data = [convert_and_pad(word_dict, test_words)[0]]\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have processed the review, we can send the resulting array to our model to predict the sentiment of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.6811264, dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the return value of our model is close to `1`, we can be certain that the review we submitted is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Of course, just like in the XGBoost notebook, once we've deployed an endpoint it continues to run until we tell it to shut down. Since we are done using our endpoint for now, we can delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 (again) - Deploy the model for the web app\n",
    "\n",
    "Now that we know that our model is working, it's time to create some custom inference code so that we can send the model a review which has not been processed and have it determine the sentiment of the review.\n",
    "\n",
    "As we saw above, by default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we now wish to accept a string as input and our model expects a processed review, we need to write some custom inference code.\n",
    "\n",
    "We will store the code that we write in the `serve` directory. Provided in this directory is the `model.py` file that we used to construct our model, a `utils.py` file which contains the `review_to_words` and `convert_and_pad` pre-processing functions which we used during the initial data processing, and `predict.py`, the file which will contain our custom inference code. Note also that `requirements.txt` is present which will tell SageMaker what Python libraries are required by our custom inference code.\n",
    "\n",
    "When deploying a PyTorch model in SageMaker, you are expected to provide four functions which the SageMaker inference container will use.\n",
    " - `model_fn`: This function is the same function that we used in the training script and it tells SageMaker how to load our model.\n",
    " - `input_fn`: This function receives the raw serialized input that has been sent to the model's endpoint and its job is to de-serialize and make the input available for the inference code.\n",
    " - `output_fn`: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the model's endpoint.\n",
    " - `predict_fn`: The heart of the inference script, this is where the actual prediction is done and is the function which you will need to complete.\n",
    "\n",
    "For the simple website that we are constructing during this project, the `input_fn` and `output_fn` methods are relatively straightforward. We only require being able to accept a string as input and we expect to return a single value as output. You might imagine though that in a more complex application the input or output may be image data or some other binary data which would require some effort to serialize.\n",
    "\n",
    "### (TODO) Writing inference code\n",
    "\n",
    "Before writing our custom inference code, we will begin by taking a look at the code which has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.word_dict = pickle.load(f)\r\n",
      "\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\r\n",
      "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\r\n",
      "\r\n",
      "    data_X, data_len = convert_and_pad(model.word_dict, review_to_words(input_data))\r\n",
      "\r\n",
      "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\r\n",
      "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\r\n",
      "    data_pack = np.hstack((data_len, data_X))\r\n",
      "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    data = torch.from_numpy(data_pack)\r\n",
      "    data = data.to(device)\r\n",
      "\r\n",
      "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\r\n",
      "    model.eval()\r\n",
      "\r\n",
      "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\r\n",
      "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        output = model.forward(data)\r\n",
      "    output = output.to(\u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    result = \u001b[36mint\u001b[39;49;00m(np.round(output))\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m result\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the `model_fn` method is the same as the one provided in the training code and the `input_fn` and `output_fn` methods are very simple and your task will be to complete the `predict_fn` method. Make sure that you save the completed file as `predict.py` in the `serve` directory.\n",
    "\n",
    "**TODO**: Complete the `predict_fn()` method in the `serve/predict.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "\n",
    "Now that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container.\n",
    "\n",
    "**NOTE**: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a `numpy` array. In our case we want to send a string so we need to construct a simple wrapper around the `RealTimePredictor` class to accomodate simple strings. In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "Now that we have deployed our model with the custom inference code, we should test to see if everything is working. Here we test our model by loading the first `250` positive and negative reviews and send them to the endpoint, then collect the results. The reason for only sending some of the data is that the amount of time it takes for our model to process the input and then perform inference is quite long and so testing the entire data set would be prohibitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # We make sure to test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                results.append(int(predictor.predict(review_input)))\n",
    "                \n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\n",
    "            # only send a small number of reviews\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting  pos  files\n",
      "Starting  neg  files\n"
     ]
    }
   ],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional test, we can try sending the `test_review` that we looked at earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'1'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our endpoint is working as expected, we can set up the web page that will interact with it. If you don't have time to finish the project now, make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 (again): Use the model for the web app\n",
    "\n",
    "> **TODO:** This entire section and the next contain tasks for you to complete, mostly using the AWS console.\n",
    "\n",
    "So far we have been accessing our model endpoint by constructing a predictor object which uses the endpoint and then just using the predictor object to perform inference. What if we wanted to create a web app which accessed our model? The way things are set up currently makes that not possible since in order to access a SageMaker endpoint the app would first have to authenticate with AWS using an IAM role which included access to SageMaker endpoints. However, there is an easier way! We just need to use some additional AWS services.\n",
    "\n",
    "<img src=\"Web App Diagram.svg\">\n",
    "\n",
    "The diagram above gives an overview of how the various services will work together. On the far right is the model which we trained above and which is deployed using SageMaker. On the far left is our web app that collects a user's movie review, sends it off and expects a positive or negative sentiment in return.\n",
    "\n",
    "In the middle is where some of the magic happens. We will construct a Lambda function, which you can think of as a straightforward Python function that can be executed whenever a specified event occurs. We will give this function permission to send and recieve data from a SageMaker endpoint.\n",
    "\n",
    "Lastly, the method we will use to execute the Lambda function is a new endpoint that we will create using API Gateway. This endpoint will be a url that listens for data to be sent to it. Once it gets some data it will pass that data on to the Lambda function and then return whatever the Lambda function returns. Essentially it will act as an interface that lets our web app communicate with the Lambda function.\n",
    "\n",
    "### Setting up a Lambda function\n",
    "\n",
    "The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we've created and then return the result.\n",
    "\n",
    "#### Part A: Create an IAM Role for the Lambda function\n",
    "\n",
    "Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the **IAM** page and click on **Roles**. Then, click on **Create role**. Make sure that the **AWS service** is the type of trusted entity selected and choose **Lambda** as the service that will use this role, then click **Next: Permissions**.\n",
    "\n",
    "In the search box type `sagemaker` and select the check box next to the **AmazonSageMakerFullAccess** policy. Then, click on **Next: Review**.\n",
    "\n",
    "Lastly, give this role a name. Make sure you use a name that you will remember later on, for example `LambdaSageMakerRole`. Then, click on **Create role**.\n",
    "\n",
    "#### Part B: Create a Lambda function\n",
    "\n",
    "Now it is time to actually create the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the AWS Lambda page and click on **Create a function**. When you get to the next page, make sure that **Author from scratch** is selected. Now, name your Lambda function, using a name that you will remember later on, for example `sentiment_analysis_func`. Make sure that the **Python 3.6** runtime is selected and then choose the role that you created in the previous part. Then, click on **Create Function**.\n",
    "\n",
    "On the next page you will see some information about the Lambda function you've just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In our example, we will use the code below. \n",
    "\n",
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }\n",
    "```\n",
    "\n",
    "Once you have copy and pasted the code above into the Lambda code editor, replace the `**ENDPOINT NAME HERE**` portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2019-11-30-01-43-18-602'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have added the endpoint name to the Lambda function, click on **Save**. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function.\n",
    "\n",
    "### Setting up API Gateway\n",
    "\n",
    "Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.\n",
    "\n",
    "Using AWS Console, navigate to **Amazon API Gateway** and then click on **Get started**.\n",
    "\n",
    "On the next page, make sure that **New API** is selected and give the new api a name, for example, `sentiment_analysis_api`. Then, click on **Create API**.\n",
    "\n",
    "Now we have created an API, however it doesn't currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.\n",
    "\n",
    "Select the **Actions** dropdown menu and click **Create Method**. A new blank method will be created, select its dropdown menu and select **POST**, then click on the check mark beside it.\n",
    "\n",
    "For the integration point, make sure that **Lambda Function** is selected and click on the **Use Lambda Proxy integration**. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "Type the name of the Lambda function you created earlier into the **Lambda Function** text entry box and then click on **Save**. Click on **OK** in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the **Actions** dropdown and click on **Deploy API**. You will need to create a new Deployment stage and name it anything you like, for example `prod`.\n",
    "\n",
    "You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text **Invoke URL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploying our web app\n",
    "\n",
    "Now that we have a publicly available API, we can start using it in a web app. For our purposes, we have provided a simple static html file which can make use of the public api you created earlier.\n",
    "\n",
    "In the `website` folder there should be a file called `index.html`. Download the file to your computer and open that file up in a text editor of your choice. There should be a line which contains **\\*\\*REPLACE WITH PUBLIC API URL\\*\\***. Replace this string with the url that you wrote down in the last step and then save the file.\n",
    "\n",
    "Now, if you open `index.html` on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.\n",
    "\n",
    "If you'd like to go further, you can host this html file anywhere you'd like, for example using github or hosting a static site on Amazon's S3. Once you have done this you can share the link with anyone you'd like and have them play with it too!\n",
    "\n",
    "> **Important Note** In order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that you are paying for it. Make sure that the endpoint is running when you want to use the web app but that you shut it down when you don't need it, otherwise you will end up with a surprisingly large AWS bill.\n",
    "\n",
    "**TODO:** Make sure that you include the edited `index.html` file in your project submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your web app is working, trying playing around with it and see how well it works.\n",
    "\n",
    "**Question**: Give an example of a review that you entered into your web app. What was the predicted sentiment of your example review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \"Sitting through a film (I'm English) for that amount of time for 10 minutes action that has a tenuous link to a non existent plot was hard work.\n",
    "\n",
    "How has this got a review rating of 8.0 when the overall consensus is its a pile of do do is beyond me!\"\n",
    "\n",
    "\n",
    "I chose this review because it was a one but I didn't see any obviously negative key words in it.  The model got it right, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
